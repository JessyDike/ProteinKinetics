{"cells":[{"cell_type":"markdown","metadata":{"id":"x2X3cMRlrQ41"},"source":["# Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7Q2Bg8ZKjrtY"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import os\n","import matplotlib.pyplot as plt\n","import matplotlib.patches as patches\n","import sys\n","import re\n","from tqdm.auto import tqdm\n","import joblib\n","import time\n","import warnings\n","import math\n","\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.metrics import mean_absolute_percentage_error, mean_squared_error, r2_score\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.nn.utils.rnn import pad_sequence\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","from torch.utils.data import Dataset, DataLoader\n","\n","from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoModel, BertTokenizer, BertModel\n","from transformers.tokenization_utils_base import BatchEncoding\n","\n","from scipy.special import exp1 # exponential integral (https://docs.scipy.org/doc/scipy/reference/generated/scipy.special.exp1.html)\n","from scipy.optimize import fsolve\n","from scipy.signal import argrelextrema\n"]},{"cell_type":"markdown","metadata":{"id":"x6gY3Fgm-5Tt"},"source":["# Load Tokenized Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FwG1R8Xj_RnI"},"outputs":[],"source":["# Specify your path to Capstone folder.\n","\n","main_path = \"/content/drive/MyDrive/Capstone_Diana/Capstone/\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n5j6DcJV_MD7"},"outputs":[],"source":["if \"drive\" in main_path:\n","    from google.colab import drive\n","    drive.mount(\"/content/drive\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v01Erc3V_aR4"},"outputs":[],"source":["# Specify your original database.\n","# mechano - for MechanoProDB;\n","# protherm - for ProThermDB\n","\n","database = \"mechano\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HmW7x-1v_Hkr"},"outputs":[],"source":["# The format of saved datasets: numeric_method.tokenization_protein.tokenization_text.dataset_length.database_name\n","# Example: tokenized_dataset_0_protbert_scibert_98_mechano\n","\n","mechano = False\n","if database == \"mechano\":\n","    numeric_method = 'regression_imputings'\n","    dataset_length = 127\n","    mechano = True\n","else:\n","    numeric_method = 'none'\n","    dataset_length = 14645\n","\n","sequence_method = \"protbert\"\n","text_method = \"scibert\"\n","\n","dataset_name = f\"tokenized_dataset_{numeric_method}_{sequence_method}_{text_method}_{dataset_length}_{database}\"\n","pickle_path = f\"{main_path}Tokenized_results/{dataset_name}.pkl\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5TpuUMTN-8tW"},"outputs":[],"source":["# Load preprocessed dataset.\n","\n","tokenized_df = pd.read_pickle(pickle_path)\n","\n","print(\"Pickle successfully loaded!\")\n","tokenized_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gWmJNqqCabxl"},"outputs":[],"source":["# The original split made for this project.\n","\n","# tokenized_df = tokenized_df.reset_index(drop=True)\n","\n","# print(\"Splitting data into train and validation sets...\")\n","# train_df, val_df = train_test_split(tokenized_df, test_size=0.2, random_state=42, shuffle=True)\n","# print(f\"Training set size: {len(train_df)}\")\n","# print(f\"Validation set size: {len(val_df)}\")\n","\n","# save_path = f\"/content/drive/MyDrive/Capstone/Tokenized_results/\"\n","# train_df.to_pickle(f\"{save_path}train_tokenized_df.pkl\")\n","# val_df.to_pickle(f\"{save_path}val_tokenized_df.pkl\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ujMOfxVYurDT"},"outputs":[],"source":["# Loading train and validation data.\n","\n","train_df = pd.read_pickle(f\"{main_path}Tokenized_results/{database}_train_tokenized_df.pkl\")\n","val_df = pd.read_pickle(f\"{main_path}Tokenized_results/{database}_val_tokenized_df.pkl\")\n","\n","train_indices = train_df.index\n","val_indices = val_df.index"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pSx65ihQskLp"},"outputs":[],"source":["# Experimental Case.\n","# If you want to use only three main columns as an input, put df_state=\"subset\", otherwise left the default one.\n","\n","df_state = \"all\" # subset/all\n","\n","if df_state == 'subset' and mechano:\n","    numeric_columns_new = [\"Pulling Start\", \"Pulling End\"]\n","    needed_cols_subset = [\"Sequence\", \"tokenized_Sequence\", \"Pulling Start\", \"Pulling End\", \"targets\", \"Experimental Conditions\", \"tokenized_Experimental Conditions\"]\n","    train_df = train_df[needed_cols_subset]\n","    val_df = val_df[needed_cols_subset]\n","    tokenized_df = tokenized_df[needed_cols_subset]\n","\n","    train_df.loc[:, \"numeric_embeddings\"] = train_df[numeric_columns_new].apply(lambda row: torch.tensor(row.values, dtype=torch.float), axis=1)\n","    val_df.loc[:, \"numeric_embeddings\"] = val_df[numeric_columns_new].apply(lambda row: torch.tensor(row.values, dtype=torch.float), axis=1)\n","    tokenized_df.loc[:, \"numeric_embeddings\"] = tokenized_df[numeric_columns_new].apply(lambda row: torch.tensor(row.values, dtype=torch.float), axis=1)\n","elif not mechano:\n","    needed_cols_subset = ['pH',  'Tm_(C)', 'Sequence', 'tokenized_Sequence',  'SEC_STR', 'tokenized_SEC_STR', 'targets', 'numeric_embeddings']\n","    train_df = train_df[needed_cols_subset]\n","    val_df = val_df[needed_cols_subset]\n","    tokenized_df = tokenized_df[needed_cols_subset]\n","else:\n","    pass"]},{"cell_type":"markdown","metadata":{"id":"Bd43FY7g4mKv"},"source":["# TensorFlow Random Forest (TFDF)"]},{"cell_type":"markdown","metadata":{"id":"iOuLBzRSNA5c"},"source":["This model was previously used only for **MechanoProDB** as is not optimal for ProThermDB usage.\n","\n","All chunks contain data related to MechanoProDB only."]},{"cell_type":"markdown","metadata":{"id":"SheOUGJ0NA5d"},"source":["## Installations and Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LNikkSp44maz"},"outputs":[],"source":["# Install TFDF and Optuna for this separate section.\n","#%pip install tensorflow==2.18\n","%pip install tensorflow tensorflow_decision_forests\n","%pip install optuna"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rlp_8cd24mdw"},"outputs":[],"source":["# Load TFDF\n","import tensorflow_decision_forests as tfdf\n","import optuna\n","import logging"]},{"cell_type":"markdown","metadata":{"id":"Sh18-CJ2NA5f"},"source":["## Functions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lHDFaFb8NA5f"},"outputs":[],"source":["def aggregate_tokenized_features(df, columns):\n","    \"\"\"\n","    Aggregates the input_ids obtained after tokenization by ProtBERT and SciBERT, using average\n","    \"\"\"\n","    for col in columns:\n","        df[col] = df[col].apply(\n","            lambda x: np.mean(x[0].ids) if hasattr(x[0], \"ids\") else 0\n","            )\n","    return df\n","\n","def relative_root_mean_squared_error(y_true, y_pred):\n","    \"\"\"\n","    Calculates the RRMSE and returns the single value\n","    \"\"\"\n","    y_true = np.asarray(y_true)\n","    y_pred = np.asarray(y_pred)\n","\n","    y_true = y_true.astype(float)\n","    y_pred = y_pred.astype(float)\n","\n","    mask = y_true != 0\n","    if np.sum(mask) == 0:\n","        return np.nan\n","\n","    y_true_filtered = y_true[mask]\n","    y_pred_filtered = y_pred[mask]\n","\n","    if len(y_true_filtered) == 0:\n","         return np.nan\n","\n","    relative_errors = 1 - (y_pred_filtered / y_true_filtered)\n","    rrmse = np.mean(np.abs(relative_errors))\n","    if np.isnan(rrmse) or np.isinf(rrmse):\n","        return np.nan\n","    return rrmse\n","\n","def evaluate_predictions(y_true, y_pred):\n","    \"\"\"\n","    Calculates and returns evaluation metrics for TFDF model\n","    \"\"\"\n","    y_true = np.asarray(y_true)\n","    y_pred = np.asarray(y_pred)\n","\n","    if y_true.ndim == 1:\n","        y_true = y_true.reshape(-1, 1)\n","    if y_pred.ndim == 1:\n","        y_pred = y_pred.reshape(-1, 1)\n","\n","    if y_true.shape != y_pred.shape:\n","         raise ValueError(f\"Shape mismatch: y_true {y_true.shape} vs y_pred {y_pred.shape}\")\n","\n","    num_outputs = y_true.shape[1]\n","    metrics_dict = {\"Output\": [], \"MAPE\": [], \"SMAPE\": [], \"MSE\": [], \"RMSE\": [], \"RRMSE\": [], \"R2\": []}\n","\n","    if np.isnan(y_pred).any():\n","        print(\"Warning: NaNs found in predictions. Metrics might be affected.\")\n","\n","    for i in range(num_outputs):\n","        y_t = y_true[:, i]\n","        y_p = y_pred[:, i]\n","\n","        mape = mean_absolute_percentage_error(y_t, y_p)\n","        mse = mean_squared_error(y_t, y_p)\n","        rmse = np.sqrt(mse)\n","        smape = 100 * np.mean(2 * np.abs(y_t - y_p) / (np.abs(y_t) + np.abs(y_p) + 1e-8))\n","        rrmse = relative_root_mean_squared_error(y_t, y_p)\n","        r2 = r2_score(y_t, y_p)\n","\n","        metrics_dict[\"Output\"].append(f\"Output_{i}\")\n","        metrics_dict[\"MAPE\"].append(mape)\n","        metrics_dict[\"SMAPE\"].append(smape)\n","        metrics_dict[\"MSE\"].append(mse)\n","        metrics_dict[\"RMSE\"].append(rmse)\n","        metrics_dict[\"RRMSE\"].append(rrmse)\n","        metrics_dict[\"R2\"].append(r2)\n","\n","    metrics_dict[\"Output\"].append(\"Average\")\n","    for metric in [\"MAPE\", \"SMAPE\", \"MSE\", \"RMSE\", \"RRMSE\", \"R2\"]:\n","        metrics_dict[metric].append(np.mean(metrics_dict[metric]))\n","\n","    return pd.DataFrame(metrics_dict)"]},{"cell_type":"markdown","metadata":{"id":"YWbwifxhNA5g"},"source":["## Dataset Processing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tNiwkcp-j0k_"},"outputs":[],"source":["# Prepare your data for usage.\n","df_tfdf = tokenized_df.copy()\n","to_drop = ['targets', 'numeric_embeddings']\n","if mechano:\n","  to_drop.extend(['tokenized_Joint_Text_Cols', 'tokenized_domain_subsequences'])\n","  df_tfdf.drop(columns = to_drop, axis = 1, inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vpoNwFwYEZJC"},"outputs":[],"source":["df_tfdf = df_tfdf.rename(columns=lambda x: x.replace(\" \", \"_\"))\n","tokenized_columns = [col for col in df_tfdf.columns if col.startswith(\"tokenized_\")]\n","df_tfdf = aggregate_tokenized_features(df_tfdf, tokenized_columns)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KLaEgoHP7gHV"},"outputs":[],"source":["if mechano:\n","    categorical_columns = [\n","        \"Name\", \"SCOP_annotation\", \"Experimental_Conditions\", \"Organism\", \"Classification\",\n","        \"Technique\", \"Pulling_Mode\", \"Unfolding_Pathway\", \"domain_subsequences\",\n","        \"Joint_Text_Cols\", \"PDB_UniProt\", \"Sequence\"\n","        ]\n","else:\n","    categorical_columns = []\n","\n","df_tfdf.drop(columns=categorical_columns, axis=1, inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mBo7HQrd-IR4"},"outputs":[],"source":["optuna.logging.get_logger(\"optuna\").addHandler(logging.StreamHandler(sys.stdout))\n","study_name = \"tfdf-regression-tuning\"\n","storage_name = f\"sqlite:///{study_name}.db\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zd5mBqumb4oH"},"outputs":[],"source":["if mechano:\n","    target_columns = [\"ΔG_[kBT]\", \"Xu_[nm]\", \"Koff_[s-¹]\"]\n","else:\n","    target_columns = []\n","\n","print(\"{} examples in training, {} examples for testing.\".format(\n","    len(train_df), len(val_df)))\n","df_tfdf = df_tfdf.rename(columns=lambda x: x.replace(\" \", \"_\").replace(\"[\", \"\").replace(\"]\", \"\").replace(\"Δ\", \"Delta\").replace(\"-¹\", \"_1\"))\n","train_df = train_df.rename(columns=lambda x: x.replace(\" \", \"_\").replace(\"[\", \"\").replace(\"]\", \"\").replace(\"Δ\", \"Delta\").replace(\"-¹\", \"_1\"))\n","val_df = val_df.rename(columns=lambda x: x.replace(\" \", \"_\").replace(\"[\", \"\").replace(\"]\", \"\").replace(\"Δ\", \"Delta\").replace(\"-¹\", \"_1\"))\n","\n","if mechano:\n","    target_columns = [\"DeltaG_kBT\", \"Xu_nm\", \"Koff_s_1\"]\n","else:\n","    target_columns = []"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dkyjlDDjCSEi"},"outputs":[],"source":["train_df.drop(columns=categorical_columns, axis=1, inplace=True)\n","val_df.drop(columns=categorical_columns, axis=1, inplace=True)\n","\n","train_df = aggregate_tokenized_features(train_df, tokenized_columns)\n","val_df = aggregate_tokenized_features(val_df, tokenized_columns)\n","\n","train_df.drop(columns=to_drop, axis=1, inplace=True)\n","val_df.drop(columns=to_drop, axis=1, inplace=True)\n","\n","feature_names = [x for x in df_tfdf.columns.tolist() if x not in target_columns]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X6bRcxjc8uoq"},"outputs":[],"source":["# Defining the Objective function for Optuna Optimization.\n","\n","def objective(trial):\n","    \"\"\"\n","    Optuna objective function to train and evaluate models.\n","    \"\"\"\n","    max_depth = trial.suggest_int(\"max_depth\", 5, 20)\n","    min_examples = trial.suggest_int(\"min_examples\", 2, 10)\n","\n","    all_target_val_rmse = []\n","    all_target_val_loss = []\n","\n","    print(f\"Optuna Trial {trial.number}\")\n","    print(f\"Hyperparameters: max_depth={max_depth}, min_examples={min_examples}\")\n","\n","    for label in target_columns:\n","        print(f\"Training for target: {label}\")\n","        missing = [f for f in feature_names + [label] if f not in train_df.columns]\n","        if missing:\n","            print(f\"Missing columns in train_df: {missing}\")\n","\n","        train_df_target = train_df[feature_names + [label]].copy()\n","        val_df_target = val_df[feature_names + [label]].copy()\n","\n","        if label not in train_df_target.columns or label not in val_df_target.columns:\n","            print(f\"Label '{label}' not found in DataFrame columns.\")\n","\n","        try:\n","            train_ds = tfdf.keras.pd_dataframe_to_tf_dataset(\n","                train_df_target, label=label, task=tfdf.keras.Task.REGRESSION, max_num_classes=100000\n","            )\n","            val_ds = tfdf.keras.pd_dataframe_to_tf_dataset(\n","                val_df_target, label=label, task=tfdf.keras.Task.REGRESSION, max_num_classes=100000\n","            )\n","        except Exception as e:\n","             print(f\"Error creating TF Dataset for target {label}: {e}\")\n","             return float('inf')\n","\n","\n","        model = tfdf.keras.RandomForestModel(\n","            task=tfdf.keras.Task.REGRESSION,\n","            features=[tfdf.keras.FeatureUsage(name=f) for f in feature_names],\n","            max_depth=max_depth,\n","            min_examples=min_examples,\n","            compute_oob_variable_importances=False,\n","            verbose=0,\n","        )\n","\n","        model.fit(train_ds, verbose=0)\n","\n","        evaluation_metrics = model.evaluate(val_ds, verbose=0, return_dict=True)\n","        print(f\"Evaluation metrics for {label}: {evaluation_metrics}\")\n","\n","        if 'root_mean_squared_error' in evaluation_metrics:\n","            val_rmse = evaluation_metrics['root_mean_squared_error']\n","        elif 'loss' in evaluation_metrics:\n","             val_rmse = np.sqrt(evaluation_metrics['loss'])\n","        else:\n","             print(f\"Warning: Could not find 'root_mean_squared_error' or 'loss' in evaluation results for {label}.\")\n","             predictions = model.predict(val_ds)\n","             y_true_np = val_df_target[label].to_numpy()\n","             val_rmse = np.sqrt(mean_squared_error(y_true_np, predictions.flatten()))\n","\n","        all_target_val_rmse.append(val_rmse)\n","        all_target_val_loss.append(evaluation_metrics.get('loss', val_rmse**2))\n","\n","    average_rmse = np.mean(all_target_val_rmse)\n","    print(f\"Trial {trial.number} Completed\")\n","    print(f\"Average Validation RMSE: {average_rmse:.4f}\")\n","    print(f\"Individual RMSEs: {all_target_val_rmse}\")\n","\n","    trial.set_user_attr(\"individual_rmses\", all_target_val_rmse)\n","    trial.set_user_attr(\"average_mse\", np.mean(all_target_val_loss))\n","\n","    return average_rmse"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"erneYP4Y9kVH","jupyter":{"outputs_hidden":true}},"outputs":[],"source":["# Start of Optuna optimization, which is done is 4 trials (can be changed).\n","\n","study = optuna.create_study(\n","    direction=\"minimize\",\n","    study_name=study_name\n","    )\n","\n","n_trials = 4\n","study.optimize(objective, n_trials=n_trials)\n","\n","print(\"Optuna Optimization Finished\")\n","print(f\"Number of finished trials: {len(study.trials)}\")\n","print(\"Best trial:\")\n","best_trial = study.best_trial"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M4vbmINeNA5k"},"outputs":[],"source":["print(f\"Value (Average Validation RMSE): {best_trial.value:.4f}\")\n","print(\"Params:\")\n","for key, value in best_trial.params.items():\n","    print(f\"{key}: {value}\")\n","print(\"  User Attributes (Example):\")\n","for key, value in best_trial.user_attrs.items():\n","    print(f\"{key}: {value}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"jZiDCefe94Lh","jupyter":{"outputs_hidden":true}},"outputs":[],"source":["# Training data with the best hyperparameters after Optuna Optimization.\n","# Getting the predictions and plotting RMSE training logs to see, which number of trees was the most optimal.\n","\n","best_params = best_trial.params\n","\n","final_evaluation_results = {}\n","all_predictions_dict = {}\n","all_true_values_dict = {}\n","\n","for label in target_columns:\n","    print(f\"\\nTraining final model for target: {label}\\n\")\n","\n","    train_df_target = train_df[feature_names + [label]].copy()\n","    val_df_target = val_df[feature_names + [label]].copy()\n","    train_ds = tfdf.keras.pd_dataframe_to_tf_dataset(train_df_target, label=label, task=tfdf.keras.Task.REGRESSION)\n","    test_ds = tfdf.keras.pd_dataframe_to_tf_dataset(val_df_target, label=label, task=tfdf.keras.Task.REGRESSION)\n","\n","    final_model = tfdf.keras.RandomForestModel(\n","        task=tfdf.keras.Task.REGRESSION,\n","        features=[tfdf.keras.FeatureUsage(name=f) for f in feature_names],\n","        max_depth=best_params.get('max_depth', 16),\n","        min_examples=best_params.get('min_examples', 5),\n","        compute_oob_variable_importances=True,\n","        verbose=1\n","        )\n","\n","    final_model.fit(train_ds)\n","    print(final_model.summary())\n","\n","    evaluation = final_model.evaluate(test_ds, return_dict=True)\n","    print(\"\\nFinal Evaluation Metrics (from model.evaluate):\")\n","    for name, value in evaluation.items():\n","      print(f\"  {name}: {value:.4f}\")\n","\n","    predictions = final_model.predict(test_ds)\n","    all_predictions_dict[label] = predictions.flatten()\n","    all_true_values_dict[label] = val_df_target[label].to_numpy()\n","\n","    try:\n","        logs = final_model.make_inspector().training_logs()\n","        if logs:\n","             plt.figure(figsize=(6, 4))\n","             metric_to_plot = 'rmse' if any(hasattr(log.evaluation, 'rmse') for log in logs) else 'loss'\n","             if any(hasattr(log.evaluation, metric_to_plot) for log in logs):\n","                 plt.plot([log.num_trees for log in logs], [getattr(log.evaluation, metric_to_plot) for log in logs])\n","                 plt.xlabel(\"Number of trees\")\n","                 plt.ylabel(f\"{metric_to_plot.upper()} (out-of-bag)\")\n","                 plt.title(f\"OOB {metric_to_plot.upper()} vs Trees for {label}\")\n","                 plt.show()\n","             else:\n","                  print(f\"Could not find '{metric_to_plot}' in training logs for {label}.\")\n","\n","        else:\n","             print(\"No training logs found.\")\n","    except Exception as e:\n","        print(f\"Could not plot training logs: {e}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nNqUzzrp606q"},"outputs":[],"source":["# Redefining renamed target columns for getting evaluation scores.\n","\n","if mechano:\n","  target_order = [\"DeltaG_kBT\", \"Xu_nm\", \"Koff_s_1\"]\n","else:\n","  target_order = []\n","y_true_matrix = np.column_stack([all_true_values_dict[t] for t in target_order])\n","y_pred_matrix = np.column_stack([all_predictions_dict[t] for t in target_order])\n","\n","final_metrics_df = evaluate_predictions(y_true_matrix, y_pred_matrix)"]},{"cell_type":"markdown","metadata":{"id":"_9keQLnUNA5m"},"source":["## Saving the Metrics Dataframe for TFDF"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O1vuDS3KNA5m"},"outputs":[],"source":["save_dir = f\"{main_path}Models_Artifacts_{database}/ML_Models/\"\n","os.makedirs(save_dir, exist_ok=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qj7HN7iNQp-a"},"outputs":[],"source":["final_metrics_df.to_csv(os.path.join(save_dir, \"metrics_df_tfdf.tsv\"), sep='\\t', index = False)\n","print(f\"Model artifacts are saved to: {os.path.abspath(save_dir)}\")"]},{"cell_type":"markdown","metadata":{"id":"VzGzYNTedhn7"},"source":["# NNs (Neural Networks)"]},{"cell_type":"markdown","metadata":{"id":"BAvJkki-7rb5"},"source":["This section should be run for Neural Networks.\n"]},{"cell_type":"markdown","metadata":{"id":"BRDd1QbnNA5o"},"source":["## Functions and Setups"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HGD6u8AV6Cnp"},"outputs":[],"source":["# Define your conditions and parameters.\n","\n","early_stopping_patience_value = 30\n","scheduler_patience_value = 2\n","scheduler_factor = 0.1\n","learning_rate = 2e-5\n","min_learning_rate = 1e-7\n","batch_size = 4\n","gradient_clip_value = 1.0\n","num_epochs = 50"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"icfOVYMt0-rF"},"outputs":[],"source":["def relative_root_mean_squared_error(y_true, y_pred):\n","    \"\"\"\n","    Computing the mean absolute relative error between predictions and non-zero true values.\n","    \"\"\"\n","    y_true = np.asarray(y_true)\n","    y_pred = np.asarray(y_pred)\n","\n","    mask = y_true != 0\n","    y_true = y_true[mask]\n","    y_pred = y_pred[mask]\n","\n","    relative_errors = 1 - (y_pred / y_true)\n","    return np.mean(np.abs(relative_errors))\n","\n","class MetricsCalculatorUpd:\n","    \"\"\"\n","    Calculates and stores evaluation metrics for training and validation.\n","    \"\"\"\n","    def __init__(self, scaler):\n","        self.scaler = scaler\n","        self.num_outputs = 0\n","\n","        if scaler is None:\n","            print(\"Warning: MetricsCalculator initialized without a target scaler.\"\n","                  \"Inverse transform for interpretable metrics (MSE, RMSE, RRMSE, R2) will not be possible.\")\n","        else:\n","            try:\n","                self.num_outputs = scaler.n_features_in_\n","                print(f\"MetricsCalculator initialized for {self.num_outputs} target outputs.\")\n","            except AttributeError:\n","                print(\"Warning: Scaler provided does not have 'n_features_in_'. Number of outputs will be inferred later.\")\n","\n","        self.metrics = {\n","            'epoch': [],\n","            'train_loss': [],\n","            'val_loss': [],\n","            'val_mse_avg': [],\n","            'val_rmse_avg': [],\n","            'val_r2_avg': [],\n","            'val_rrmse_avg': []\n","        }\n","\n","        if self.num_outputs > 0:\n","            self._initialize_per_output_metrics()\n","        else:\n","            self._per_output_initialized = False\n","        self._reset_epoch_accumulators()\n","\n","    def _initialize_per_output_metrics(self):\n","        \"\"\"\n","        Initializes dictionary keys for per-output metrics.\n","        \"\"\"\n","        if self.num_outputs <= 0:\n","             print(\"Warning: Cannot initialize per-output metrics without knowing the number of outputs.\")\n","             return\n","        for i in range(self.num_outputs):\n","            self.metrics[f'val_mse_{i}'] = []\n","            self.metrics[f'val_rmse_{i}'] = []\n","            self.metrics[f'val_r2_{i}'] = []\n","            self.metrics[f'val_rrmse_{i}'] = []\n","        self._per_output_initialized = True\n","        print(f\"Initialized metric keys for {self.num_outputs} individual outputs.\")\n","\n","    def _reset_epoch_accumulators(self):\n","        self.epoch_train_loss, self.epoch_train_samples = 0.0, 0\n","        self.epoch_val_loss, self.epoch_val_samples = 0.0, 0\n","        self.epoch_val_preds_scaled, self.epoch_val_targets_scaled, self.epoch_val_targets_original = [], [], []\n","\n","    def update_train_batch(self, loss_item, batch_size):\n","        self.epoch_train_loss += loss_item * batch_size\n","        self.epoch_train_samples += batch_size\n","\n","    def update_val_batch(self, loss_item, predictions_scaled, targets_scaled, targets_original, batch_size):\n","        self.epoch_val_loss += loss_item * batch_size\n","        self.epoch_val_samples += batch_size\n","\n","        preds_np = predictions_scaled.detach().cpu().numpy()\n","        targets_scaled_np = targets_scaled.detach().cpu().numpy()\n","        if isinstance(targets_original, torch.Tensor):\n","            targets_original_np = targets_original.detach().cpu().numpy()\n","        elif isinstance(targets_original, np.ndarray):\n","             targets_original_np = targets_original\n","        else:\n","             try:\n","                 targets_original_np = np.asarray(targets_original)\n","             except Exception as e:\n","                 print(f\"Warning: Could not convert targets_original to numpy array: {e}\")\n","                 targets_original_np = None\n","\n","        self.epoch_val_preds_scaled.append(preds_np)\n","        self.epoch_val_targets_scaled.append(targets_scaled_np)\n","        if targets_original_np is not None:\n","             self.epoch_val_targets_original.append(targets_original_np)\n","\n","    def calculate_epoch_metrics(self, epoch):\n","        \"\"\"\n","        Calculates average and per-output metrics for the completed epoch and returns a dictionary.\n","        \"\"\"\n","        train_loss = self.epoch_train_loss / self.epoch_train_samples if self.epoch_train_samples > 0 else 0.0\n","        val_loss = self.epoch_val_loss / self.epoch_val_samples if self.epoch_val_samples > 0 else 0.0\n","\n","        val_mse_avg, val_rmse_avg, val_r2_avg, val_rrmse_avg = np.nan, np.nan, np.nan, np.nan\n","        list_val_mse, list_val_rmse, list_val_r2, list_val_rrmse = [], [], [], []\n","\n","        if self.epoch_val_preds_scaled and self.scaler is not None and self.epoch_val_targets_original:\n","            try:\n","                all_preds_scaled = np.concatenate(self.epoch_val_preds_scaled, axis=0)\n","                all_targets_original = np.concatenate(self.epoch_val_targets_original, axis=0)\n","\n","                if self.num_outputs == 0:\n","                    self.num_outputs = all_targets_original.shape[1]\n","                    print(f\"Inferred number of outputs: {self.num_outputs}\")\n","                    if not self._per_output_initialized:\n","                         self._initialize_per_output_metrics()\n","                elif all_targets_original.shape[1] != self.num_outputs:\n","                     print(f\"Warning: Data shape mismatch! Expected {self.num_outputs} outputs, got {all_targets_original.shape[1]}. Re-initializing metrics.\")\n","                     self.num_outputs = all_targets_original.shape[1]\n","                     self._initialize_per_output_metrics()\n","\n","                all_preds_original = np.full_like(all_targets_original, np.nan)\n","                if all_preds_scaled.shape[1] == self.scaler.n_features_in_:\n","                    all_preds_original = self.scaler.inverse_transform(all_preds_scaled)\n","                else:\n","                    print(f\"Warning: Shape mismatch during inverse transform. Preds shape[1]: {all_preds_scaled.shape[1]}, Scaler expects: {self.scaler.n_features_in_}.\")\n","\n","            except Exception as e:\n","                 print(f\"Error during inverse transform or concatenation: {e}\")\n","                 all_preds_original = np.full_like(all_targets_original, np.nan)\n","\n","            if not np.isnan(all_preds_original).any() and self.num_outputs > 0:\n","                try:\n","                    val_mse_avg = mean_squared_error(all_targets_original, all_preds_original)\n","                    val_rmse_avg = np.sqrt(val_mse_avg)\n","                    val_r2_avg = r2_score(all_targets_original, all_preds_original)\n","                    val_rrmse_avg = relative_root_mean_squared_error(all_targets_original, all_preds_original)\n","\n","                    for i in range(self.num_outputs):\n","                        target_i = all_targets_original[:, i]\n","                        pred_i = all_preds_original[:, i]\n","                        mse_i = mean_squared_error(target_i, pred_i)\n","                        rmse_i = np.sqrt(mse_i)\n","                        rrmse_i = relative_root_mean_squared_error(target_i, pred_i)\n","                        try:\n","                            r2_i = r2_score(target_i, pred_i)\n","                        except ValueError:\n","                            print(f\"Warning: R2 score could not be calculated for output {i} (constant target?).\")\n","                            r2_i = np.nan\n","\n","                        list_val_mse.append(mse_i)\n","                        list_val_rmse.append(rmse_i)\n","                        list_val_r2.append(r2_i)\n","                        list_val_rrmse.append(rrmse_i)\n","\n","                except Exception as e:\n","                    print(f\"Error calculating metrics: {e}\")\n","                    val_mse_avg, val_rmse_avg, val_r2_avg, val_rrmse_avg = np.nan, np.nan, np.nan, np.nan\n","                    list_val_mse, list_val_rmse, list_val_r2, list_val_rrmse = [], [], [], []\n","            else:\n","                 print(\"Metrics (MSE, RMSE, R2, RRMSE) on original scale could not be calculated (NaN predictions or zero outputs).\")\n","\n","        self.metrics['epoch'].append(epoch + 1)\n","        self.metrics['train_loss'].append(train_loss)\n","        self.metrics['val_loss'].append(val_loss)\n","        self.metrics['val_mse_avg'].append(val_mse_avg)\n","        self.metrics['val_rmse_avg'].append(val_rmse_avg)\n","        self.metrics['val_r2_avg'].append(val_r2_avg)\n","        self.metrics['val_rrmse_avg'].append(val_rrmse_avg)\n","\n","        if self._per_output_initialized:\n","             for i in range(self.num_outputs):\n","                 self.metrics[f'val_mse_{i}'].append(list_val_mse[i] if i < len(list_val_mse) else np.nan)\n","                 self.metrics[f'val_rmse_{i}'].append(list_val_rmse[i] if i < len(list_val_rmse) else np.nan)\n","                 self.metrics[f'val_r2_{i}'].append(list_val_r2[i] if i < len(list_val_r2) else np.nan)\n","                 self.metrics[f'val_rrmse_{i}'].append(list_val_rrmse[i] if i < len(list_val_rrmse) else np.nan)\n","        elif self.num_outputs > 0 and list_val_mse:\n","            self._initialize_per_output_metrics()\n","            for i in range(self.num_outputs):\n","                 self.metrics[f'val_mse_{i}'].append(list_val_mse[i] if i < len(list_val_mse) else np.nan)\n","                 self.metrics[f'val_rmse_{i}'].append(list_val_rmse[i] if i < len(list_val_rmse) else np.nan)\n","                 self.metrics[f'val_r2_{i}'].append(list_val_r2[i] if i < len(list_val_r2) else np.nan)\n","                 self.metrics[f'val_rrmse_{i}'].append(list_val_rrmse[i] if i < len(list_val_rrmse) else np.nan)\n","\n","        returned_train_loss = train_loss\n","        returned_val_loss = val_loss\n","        returned_val_mse_avg = val_mse_avg\n","        returned_val_rmse_avg = val_rmse_avg\n","        returned_val_r2_avg = val_r2_avg\n","        returned_val_rrmse_avg = val_rrmse_avg\n","        returned_list_mse = list_val_mse\n","        returned_list_rmse = list_val_rmse\n","        returned_list_r2 = list_val_r2\n","        returned_list_rrmse = list_val_rrmse\n","\n","        self._reset_epoch_accumulators()\n","\n","        return (returned_train_loss, returned_val_loss,\n","                returned_val_mse_avg, returned_val_rmse_avg, returned_val_r2_avg, returned_val_rrmse_avg,\n","                returned_list_mse, returned_list_rmse, returned_list_r2, returned_list_rrmse)\n","\n","    def get_metrics_df(self):\n","        \"\"\"\n","        Returns the dataframe, containing all calculated metrics.\n","        \"\"\"\n","        if not self.metrics['epoch']: return pd.DataFrame(self.metrics)\n","\n","        max_len = len(self.metrics['epoch'])\n","        for key, value in self.metrics.items():\n","            if len(value) < max_len:\n","                padding = [np.nan] * (max_len - len(value))\n","                self.metrics[key] = value + padding\n","\n","        return pd.DataFrame(self.metrics)\n","\n","    def get_last_comparison_batch(self):\n","        \"\"\"\n","        Provides the latest batch of model predictions and actual target values.\n","        \"\"\"\n","        if not self.epoch_val_preds_scaled or self.scaler is None: return None, None\n","        try:\n","            last_preds_scaled = self.epoch_val_preds_scaled[-1] if self.epoch_val_preds_scaled else None\n","            last_targets_original = self.epoch_val_targets_original[-1] if self.epoch_val_targets_original else None\n","\n","            if last_preds_scaled is None or last_targets_original is None: return None, None\n","\n","            if last_preds_scaled.shape[1] != self.scaler.n_features_in_:\n","                print(f\"Warning: Shape mismatch in get_last_comparison_batch.\")\n","                last_preds_original = np.full_like(last_targets_original, np.nan)\n","            else:\n","                last_preds_original = self.scaler.inverse_transform(last_preds_scaled)\n","            return last_preds_original, last_targets_original\n","        except Exception as e:\n","            print(f\"Error getting last comparison batch: {e}\")\n","            return None, None\n","\n","def collate_fn(batch):\n","    collated = {}\n","    collated['numeric_features'] = torch.stack([item['numeric_features'] for item in batch])\n","    collated['targets_scaled'] = torch.stack([item['targets_scaled'] for item in batch])\n","    collated['targets_original'] = torch.stack([item['targets_original'] for item in batch])\n","\n","    num_tokenized_cols = len(batch[0]['input_ids'])\n","    collated['input_ids'] = []\n","    collated['attention_mask'] = []\n","    for i in range(num_tokenized_cols):\n","        collated['input_ids'].append(torch.stack([item['input_ids'][i] for item in batch]))\n","        collated['attention_mask'].append(torch.stack([item['attention_mask'][i] for item in batch]))\n","\n","    return collated"]},{"cell_type":"markdown","metadata":{"id":"O97jBBn3NA5q"},"source":["## Model Choice"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BSUoLTmsNA51"},"outputs":[],"source":["# Specify your model below.\n","# 1. ESM2\n","# 2. BERT - for combined ProtBERT and SciBERT and Regression Head\n","# 3. BiLSTM - for BiLSTM model, using ProtBERT and SciBERT tokenizations\n","\n","model_chosen = \"ESM2\""]},{"cell_type":"markdown","metadata":{"id":"i8-gzBXzNA51"},"source":["## Processing Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"52zYDSlzOsWG"},"outputs":[],"source":["df = tokenized_df.copy()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cUpNY9g6I_OO"},"outputs":[],"source":["# Dropping these columns as they were not utilized in the scope of this project.\n","\n","if 'tokenized_Joint_Text_Cols' in df.columns:\n","    df.drop(columns=['tokenized_Joint_Text_Cols', 'tokenized_domain_subsequences'], inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0OND2npqNA53"},"outputs":[],"source":["dataset_text_columns = [x.replace(\"tokenized_\", \"\") for x in df.columns if \"tokenized_\" in x]\n","if model_chosen == \"BiLSTM\":\n","    prot_col_list = [\"tokenized_Sequence\"]\n","    print(f\"Protein column: {prot_col_list[0]}\")\n","elif model_chosen == \"BERT\":\n","    prot_col_list = ['Sequence']\n","    print(f\"Protein column: {prot_col_list[0]}\")\n","sci_col_list = [col.replace(\"tokenized_\", \"\") for col in df.columns if col.startswith(\"tokenized_\") and col != \"tokenized_Sequence\"]\n","\n","original_numeric_col = \"numeric_embeddings\"\n","target_col = \"targets\"\n","\n","print(f\"Scientific text columns: {sci_col_list}\")\n","print(f\"Original numeric column: {original_numeric_col}\")\n","print(f\"Original target column: {target_col}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i00iI7Yx6zzv"},"outputs":[],"source":["# Getting the lengths for features to input into the models.\n","# Works both for MechanoProDB and ProThermDB.\n","\n","len_targets = len(tokenized_df.targets.iloc[0])\n","len_numeric = len(tokenized_df.numeric_embeddings.iloc[0])\n","\n","len_prot_cols = 1\n","len_sci_cols = len(sci_col_list)\n","len_text_cols_all = len_prot_cols + len_sci_cols"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ffxz88WGbp5p"},"outputs":[],"source":["# Instantiating numeric and target scalers for scaling.\n","\n","numeric_scaler = StandardScaler()\n","target_scaler = MinMaxScaler(feature_range=(0, 1))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2mPacUkaNA53"},"outputs":[],"source":["numeric_features_all = np.stack(df[original_numeric_col].values)\n","print(f\"Numeric features shape: {numeric_features_all.shape}\")\n","\n","numeric_features_train = numeric_features_all[train_indices]\n","numeric_features_val = numeric_features_all[val_indices]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U-oyIdQbNA54"},"outputs":[],"source":["numeric_features_train_scaled = numeric_scaler.fit_transform(numeric_features_train)\n","numeric_features_val_scaled = numeric_scaler.transform(numeric_features_val)\n","print(\"Numeric features are scaled.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xJIjuJDhNA54"},"outputs":[],"source":["scaled_numeric_col_name = \"numeric_features_scaled\"\n","train_df[scaled_numeric_col_name] = [row for row in numeric_features_train_scaled]\n","val_df[scaled_numeric_col_name] = [row for row in numeric_features_val_scaled]\n","print(f\"Scaled numeric features added to DataFrames under column: {scaled_numeric_col_name}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vJTu5r1PNA55"},"outputs":[],"source":["print(\"Scaling target variables using MinMaxScaler...\")\n","train_targets_original = np.stack(train_df[target_col].values)\n","val_targets_original = np.stack(val_df[target_col].values)\n","train_targets_scaled = target_scaler.fit_transform(train_targets_original)\n","val_targets_scaled = target_scaler.transform(val_targets_original)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aHJtW7eaNA55"},"outputs":[],"source":["scaled_target_col_name = 'targets_scaled'\n","original_target_col_name = 'targets_original'\n","train_df[scaled_target_col_name] = [torch.tensor(row, dtype=torch.float32) for row in train_targets_scaled]\n","train_df[original_target_col_name] = [torch.tensor(row, dtype=torch.float32) for row in train_targets_original]\n","val_df[scaled_target_col_name] = [torch.tensor(row, dtype=torch.float32) for row in val_targets_scaled]\n","val_df[original_target_col_name] = [torch.tensor(row, dtype=torch.float32) for row in val_targets_original]\n","print(\"Scaled and original targets added to DataFrames.\")"]},{"cell_type":"markdown","metadata":{"id":"FX0iWRVgNA55"},"source":["## Models"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iQAmuuJONA56"},"outputs":[],"source":["# Selecting available GPU or CPU if unavailable.\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(f\"Using device: {device}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ssau5hg_NA56"},"outputs":[],"source":["# Loading and configuring pretrained models for chosen options.\n","# Some models are freezed or partially unfreezed for future training.\n","\n","if model_chosen == \"ESM2\":\n","    model_name = \"facebook/esm2_t6_8M_UR50D\"\n","    tokenizer = AutoTokenizer.from_pretrained(model_name)\n","    model = AutoModel.from_pretrained(model_name)\n","\n","    output_size = model.config.hidden_size\n","    print(f\"Hidden size: {output_size}\")\n","\n","    for param in model.parameters():\n","        param.requires_grad = False\n","\n","    # Unfreezing the last two layers of the encoder for ESM2\n","    for param in model.encoder.layer[-2:].parameters():\n","        param.requires_grad = True\n","\n","elif model_chosen in [\"BERT\", \"BiLSTM\"]:\n","    prot_model_name = \"Rostlab/prot_bert_bfd\"\n","    sci_model_name = \"allenai/scibert_scivocab_uncased\"\n","\n","    prot_tokenizer = AutoTokenizer.from_pretrained(prot_model_name)\n","    prot_model = AutoModel.from_pretrained(prot_model_name)\n","    prot_model.to(device)\n","\n","    sci_tokenizer = AutoTokenizer.from_pretrained(sci_model_name)\n","    sci_model = AutoModel.from_pretrained(sci_model_name)\n","    sci_model.to(device)\n","\n","if model_chosen == \"BERT\":\n","    for param in prot_model.parameters():\n","        param.requires_grad = False\n","    for param in sci_model.parameters():\n","        param.requires_grad = False"]},{"cell_type":"markdown","metadata":{"id":"vv4W0Lxzom2A"},"source":["### Using Mean Pooling in AddedSublayer (Run for ESM2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VhJskImon0M_"},"outputs":[],"source":["class AddedSubLayer(nn.Module):\n","    \"\"\"\n","    Class definition for Regression Head, which takes the sequences, furtherly averaging them through Mean Pooling.\n","    \"\"\"\n","    def __init__(self, pretrained_model, num_numeric_features, output_size=len_targets, num_tokenized_cols=len_text_cols_all):\n","        super(AddedSubLayer, self).__init__()\n","        self.esm2 = pretrained_model\n","        self.esm2_hidden_size = pretrained_model.config.hidden_size\n","        self.num_tokenized_cols = num_tokenized_cols\n","        self.fc_input_size = self.esm2_hidden_size * self.num_tokenized_cols + num_numeric_features\n","        self.fc = nn.Linear(self.fc_input_size, output_size)\n","        self.dropout = nn.Dropout(p=0.1)\n","\n","    def _mean_pooling(self, model_output, attention_mask):\n","        \"\"\"\n","        Computes mean-pooled token embeddings by averaging the hidden states while masking out padding tokens.\n","        \"\"\"\n","        token_embeddings = model_output.last_hidden_state\n","        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n","        sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n","        sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n","        return sum_embeddings / sum_mask\n","\n","    def forward(self, input_ids_list, attention_mask_list, numeric_features):\n","        \"\"\"\n","        Processes multiple tokenized inputs and numeric features through the model.\n","        Outputs predictions using Linear layer and Sigmoid activation function.\n","        \"\"\"\n","        pooled_outputs = []\n","        assert len(input_ids_list) == self.num_tokenized_cols, f\"Expected {self.num_tokenized_cols} input sequences, got {len(input_ids_list)}\"\n","        assert len(attention_mask_list) == self.num_tokenized_cols, f\"Expected {self.num_tokenized_cols} attention masks, got {len(attention_mask_list)}\"\n","\n","        for i in range(self.num_tokenized_cols):\n","            outputs = self.esm2(input_ids=input_ids_list[i], attention_mask=attention_mask_list[i])\n","            pooled_output = self._mean_pooling(outputs, attention_mask_list[i])\n","            pooled_outputs.append(pooled_output)\n","        concatenated_pooled_output = torch.cat(pooled_outputs, dim=1)\n","\n","        combined_features = torch.cat((concatenated_pooled_output, numeric_features), dim=1)\n","        combined_features = self.dropout(combined_features)\n","        logits = self.fc(combined_features)\n","        predictions_scaled = torch.sigmoid(logits)\n","        return predictions_scaled"]},{"cell_type":"markdown","metadata":{"id":"FKVz8uJWsWHZ"},"source":["### Using All sequences in AddedSublayer (Run for ESM2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J359PZnUouwG"},"outputs":[],"source":["# Added SubLayer without Pooling applied for text features (performed worse than Mean Pooling).\n","\n","# class AddedSubLayer(nn.Module):\n","#     \"\"\"\n","#     Class definition for Regression Head, which takes the sequences directly, without pooling them.\n","#     \"\"\"\n","#     def __init__(self, pretrained_model, num_numeric_features, output_size=len_targets, num_tokenized_cols=len_text_cols_all, max_length=1024):\n","#         super(AddedSubLayer, self).__init__()\n","#         self.esm2 = pretrained_model\n","#         self.esm2_hidden_size = pretrained_model.config.hidden_size\n","#         self.num_tokenized_cols = num_tokenized_cols\n","#         self.max_length = max_length\n","\n","#         flattened_text_size_per_sequence = self.max_length * self.esm2_hidden_size\n","#         total_flattened_text_size = flattened_text_size_per_sequence * self.num_tokenized_cols\n","#         self.fc_input_size = total_flattened_text_size + num_numeric_features\n","\n","#         print(f\"WARNING: Final Linear layer input size = {self.fc_input_size} (Flattened Text: {total_flattened_text_size}, Numeric: {num_numeric_features})\")\n","#         self.fc = nn.Linear(self.fc_input_size, output_size)\n","#         self.dropout = nn.Dropout(p=0.1)\n","\n","#     def forward(self, input_ids_list, attention_mask_list, numeric_features):\n","#         \"\"\"\n","#         Flattens the full hidden states of each tokenized input, concatenating the results witjh numeric features.\n","#         Produces Sigoid-scaled predictions.\n","#         \"\"\"\n","#         assert len(input_ids_list) == self.num_tokenized_cols, f\"Expected {self.num_tokenized_cols} input sequences, got {len(input_ids_list)}\"\n","#         assert len(attention_mask_list) == self.num_tokenized_cols, f\"Expected {self.num_tokenized_cols} attention masks, got {len(attention_mask_list)}\"\n","#         batch_size = numeric_features.shape[0]\n","#         flattened_outputs = []\n","\n","#         for i in range(self.num_tokenized_cols):\n","#             outputs = self.esm2(input_ids=input_ids_list[i], attention_mask=attention_mask_list[i])\n","#             flattened_output = outputs.last_hidden_state.contiguous().view(batch_size, -1)\n","#             expected_flat_size = self.max_length * self.esm2_hidden_size\n","#             if flattened_output.shape[1] != expected_flat_size:\n","#                 raise RuntimeError(f\"Flattened size mismatch for input {i}. \"\n","#                                     f\"Expected {expected_flat_size}, got {flattened_output.shape[1]}. \"\n","#                                     f\"Check max_length used in tokenizer vs. model init.\")\n","\n","#             flattened_outputs.append(flattened_output)\n","\n","#         concatenated_flattened_text = torch.cat(flattened_outputs, dim=1)\n","#         combined_features = torch.cat((concatenated_flattened_text, numeric_features), dim=1)\n","#         combined_features = self.dropout(combined_features)\n","#         logits = self.fc(combined_features)\n","#         predictions_scaled = torch.sigmoid(logits)\n","#         return predictions_scaled"]},{"cell_type":"markdown","metadata":{"id":"9M_-6BZwo18M"},"source":["### Continue with ESM2 model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ayJtz71Jn0Rc"},"outputs":[],"source":["class ProteinDatasetESM2(Dataset):\n","    def __init__(self, df, text_cols, num_feat_col=\"numeric_embeddings\",\n","                 scaled_target_col=\"targets_scaled\", original_target_col=\"targets_original\",\n","                 tokenizer=None, max_length=1024):\n","      self.text_cols = text_cols\n","      self.numeric_features = torch.stack([torch.tensor(item, dtype=torch.float32) for item in df[num_feat_col].tolist()], dim=0)\n","      self.targets_scaled = torch.tensor(np.stack(df[scaled_target_col].values), dtype=torch.float32)\n","      self.targets_original = torch.tensor(np.stack(df[original_target_col].values), dtype=torch.float32)\n","      self.tokenizer = tokenizer\n","      self.max_length = max_length\n","      self.num_samples = len(self.targets_scaled)\n","      self.num_tokenized_cols = len(self.text_cols)\n","      self.text_dict = {}\n","      print(\"Loading text data...\")\n","      for col in self.text_cols:\n","          if col not in df.columns:\n","                raise ValueError(f\"Text column '{col}' not found in DataFrame.\")\n","          self.text_dict[col] = df[col].fillna('').astype(str).tolist()\n","\n","      print(\"Validating data lengths...\")\n","      if len(self.numeric_features) != self.num_samples:\n","            raise ValueError(f\"Mismatch in length between numeric features ({len(self.numeric_features)}) and targets ({self.num_samples})\")\n","      if len(self.targets_original) != self.num_samples:\n","             raise ValueError(f\"Mismatch in length between original targets ({len(self.targets_original)}) and scaled targets ({self.num_samples})\")\n","      for col in self.text_cols:\n","          if len(self.text_dict[col]) != self.num_samples:\n","              raise ValueError(f\"Mismatch in length between texts in column '{col}' ({len(self.text_dict[col])}) and targets ({self.num_samples})\")\n","\n","    def __len__(self):\n","        \"\"\"\n","        Returns the number of samples in the dataset.\n","        \"\"\"\n","        return self.num_samples\n","\n","    def __getitem__(self, idx):\n","        \"\"\"\n","        Retrieves a single data sample by index.\n","        \"\"\"\n","        data = {\n","            \"numeric_features\": self.numeric_features[idx],\n","            \"targets_scaled\": self.targets_scaled[idx],\n","            \"targets_original\": self.targets_original[idx],\n","            \"input_ids\": [],\n","            \"attention_mask\": []\n","        }\n","\n","        for col in self.text_cols:\n","            sequence = self.text_dict[col][idx]\n","            inputs = self.tokenizer(\n","                sequence,\n","                max_length=self.max_length,\n","                padding='max_length',\n","                truncation=True,\n","                return_tensors=\"pt\"\n","            )\n","            data[\"input_ids\"].append(inputs['input_ids'].squeeze(0))\n","            data[\"attention_mask\"].append(inputs['attention_mask'].squeeze(0))\n","\n","        return data\n"]},{"cell_type":"markdown","metadata":{"id":"RG80MaK7NA5-"},"source":["### BERT Model Classes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3e_YP66xNA5-"},"outputs":[],"source":["class ProteinDatasetBERT(Dataset):\n","    def __init__(self, df,\n","                 prot_text_cols,\n","                 sci_text_cols,\n","                 prot_tokenizer,\n","                 sci_tokenizer,\n","                 num_feat_col=\"numeric_embeddings\",\n","                 scaled_target_col=\"targets_scaled\",\n","                 original_target_col=\"targets_original\",\n","                 max_length_prot=1024,\n","                 max_length_sci=512):\n","      self.prot_text_cols = prot_text_cols\n","      self.sci_text_cols = sci_text_cols\n","      self.prot_tokenizer = prot_tokenizer\n","      self.sci_tokenizer = sci_tokenizer\n","      self.max_length_prot = max_length_prot\n","      self.max_length_sci = max_length_sci\n","\n","      if len(prot_text_cols) != 1:\n","          raise ValueError(f\"Expected exactly 1 protein text column, got {len(prot_text_cols)}\")\n","\n","      self.all_text_cols = self.prot_text_cols + self.sci_text_cols\n","\n","      self.numeric_features = torch.stack([torch.tensor(item, dtype=torch.float32) for item in df[num_feat_col].tolist()], dim=0)\n","      self.targets_scaled = torch.tensor(np.stack(df[scaled_target_col].values), dtype=torch.float32)\n","      self.targets_original = torch.tensor(np.stack(df[original_target_col].values), dtype=torch.float32)\n","      self.num_samples = len(self.targets_scaled)\n","\n","      self.text_dict = {}\n","      print(\"Loading text data...\")\n","      for col in self.all_text_cols:\n","          if col not in df.columns:\n","                raise ValueError(f\"Text column '{col}' not found in DataFrame.\")\n","          self.text_dict[col] = df[col].fillna('').astype(str).tolist()\n","\n","      print(\"Validating data lengths...\")\n","      if len(self.numeric_features) != self.num_samples:\n","            raise ValueError(f\"Mismatch in length between numeric features ({len(self.numeric_features)}) and targets ({self.num_samples})\")\n","      if len(self.targets_original) != self.num_samples:\n","             raise ValueError(f\"Mismatch in length between original targets ({len(self.targets_original)}) and scaled targets ({self.num_samples})\")\n","      for col in self.all_text_cols:\n","          if len(self.text_dict[col]) != self.num_samples:\n","              raise ValueError(f\"Mismatch in length between texts in column '{col}' ({len(self.text_dict[col])}) and targets ({self.num_samples})\")\n","\n","    def __len__(self):\n","        \"\"\"\n","        Returns the number of samples in the dataset.\n","        \"\"\"\n","        return self.num_samples\n","\n","    def __getitem__(self, idx):\n","        \"\"\"\n","        Retrieves a single sample by index.\n","        \"\"\"\n","        data = {\n","            \"numeric_features\": self.numeric_features[idx],\n","            \"targets_scaled\": self.targets_scaled[idx],\n","            \"targets_original\": self.targets_original[idx],\n","            \"input_ids\": [],\n","            \"attention_mask\": []\n","        }\n","\n","        for col in self.prot_text_cols:\n","            sequence = self.text_dict[col][idx]\n","            inputs = self.prot_tokenizer(\n","                sequence,\n","                max_length=self.max_length_prot,\n","                padding='max_length',\n","                truncation=True,\n","                return_tensors=\"pt\"\n","            )\n","            data[\"input_ids\"].append(inputs['input_ids'].squeeze(0))\n","            data[\"attention_mask\"].append(inputs['attention_mask'].squeeze(0))\n","\n","        for col in self.sci_text_cols:\n","            sequence = self.text_dict[col][idx]\n","            inputs = self.sci_tokenizer(\n","                sequence,\n","                max_length=self.max_length_sci,\n","                padding='max_length',\n","                truncation=True,\n","                return_tensors=\"pt\"\n","            )\n","            data[\"input_ids\"].append(inputs['input_ids'].squeeze(0))\n","            data[\"attention_mask\"].append(inputs['attention_mask'].squeeze(0))\n","\n","        return data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6vItfsnINA5_"},"outputs":[],"source":["class BertModel(nn.Module):\n","    def __init__(self, prot_model, sci_model, num_numeric_features, output_size=len_targets,\n","                 num_prot_cols=len_prot_cols, num_sci_cols=len_sci_cols, dropout_rate=0.1):\n","        super(BertModel, self).__init__()\n","\n","        if num_prot_cols != 1:\n","             raise ValueError(\"This implementation expects exactly 1 protein column.\")\n","\n","        self.prot_model = prot_model\n","        self.sci_model = sci_model\n","        self.num_prot_cols = num_prot_cols\n","        self.num_sci_cols = num_sci_cols\n","\n","        try:\n","             self.prot_hidden_size = prot_model.config.hidden_size\n","        except AttributeError:\n","             if hasattr(prot_model, 'embed_dim'):\n","                 self.prot_hidden_size = prot_model.embed_dim\n","             elif hasattr(prot_model, 'hidden_size'):\n","                 self.prot_hidden_size = prot_model.hidden_size\n","             else:\n","                 raise AttributeError(\"Could not determine hidden size for prot_model. Check model structure.\")\n","\n","        try:\n","             self.sci_hidden_size = sci_model.config.hidden_size\n","        except AttributeError:\n","             if hasattr(sci_model, 'hidden_size'):\n","                 self.sci_hidden_size = sci_model.hidden_size\n","             else:\n","                 raise AttributeError(\"Could not determine hidden size for sci_model. Check model structure.\")\n","\n","        self.fc_input_size = self.prot_hidden_size + self.sci_hidden_size + num_numeric_features\n","        self.norm_prot = nn.LayerNorm(self.prot_hidden_size)\n","        # self.norm_sci = nn.LayerNorm(self.sci_hidden_size)\n","        # self.norm_numeric = nn.LayerNorm(num_numeric_features)\n","        self.dropout = nn.Dropout(p=dropout_rate)\n","        self.fc = nn.Linear(self.fc_input_size, output_size)\n","\n","    def _mean_pooling(self, model_output, attention_mask):\n","        \"\"\"\n","        Computes the mean of token embeddings across the sequence length, weighted by the attention mask.\n","        \"\"\"\n","        if hasattr(model_output, 'last_hidden_state'):\n","            token_embeddings = model_output.last_hidden_state\n","        else:\n","             token_embeddings = model_output\n","\n","        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n","        sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n","        sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n","        return sum_embeddings / sum_mask\n","\n","    def forward(self, input_ids_list, attention_mask_list, numeric_features):\n","        \"\"\"\n","        Processes one protein input and multiple scientific text inputs through their models.\n","        Pools and combines the results with numeric features, then predicts using a linear layer with sigmoid activation.\n","        \"\"\"\n","        total_expected_cols = self.num_prot_cols + self.num_sci_cols\n","        assert len(input_ids_list) == total_expected_cols, f\"Expected {total_expected_cols} input sequences, got {len(input_ids_list)}\"\n","        assert len(attention_mask_list) == total_expected_cols, f\"Expected {total_expected_cols} attention masks, got {len(attention_mask_list)}\"\n","\n","        prot_outputs = self.prot_model(input_ids=input_ids_list[0], attention_mask=attention_mask_list[0])\n","        prot_pooled = self._mean_pooling(prot_outputs, attention_mask_list[0])\n","        prot_pooled = self.norm_prot(prot_pooled)\n","\n","        sci_pooled_outputs = []\n","        for i in range(self.num_prot_cols, total_expected_cols):\n","            sci_outputs = self.sci_model(input_ids=input_ids_list[i], attention_mask=attention_mask_list[i])\n","            sci_pooled = self._mean_pooling(sci_outputs, attention_mask_list[i])\n","            sci_pooled_outputs.append(sci_pooled)\n","\n","        sci_pooled_stacked = torch.stack(sci_pooled_outputs, dim=0)\n","        sci_pooled_avg = torch.mean(sci_pooled_stacked, dim=0)\n","\n","        combined_features = torch.cat((prot_pooled, sci_pooled_avg, numeric_features), dim=1)\n","        combined_features = self.dropout(combined_features)\n","\n","        logits = self.fc(combined_features)\n","        predictions_scaled = torch.sigmoid(logits)\n","\n","        return predictions_scaled"]},{"cell_type":"markdown","metadata":{"id":"XiZoDo7PNA6A"},"source":["### BiLSTM Model classes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-IBJlnyzNA6A"},"outputs":[],"source":["class MultiInputBiLSTMRegressor(nn.Module):\n","    def __init__(self, protbert_model, scibert_model, num_lstm_layers=2, lstm_dropout=0.2, final_dropout=0.3, hidden_size=128,\n","                 num_numeric_features=len_numeric, output_size=len_targets, len_prot_cols = len_prot_cols, len_sci_cols = len_sci_cols):\n","        super(MultiInputBiLSTMRegressor, self).__init__()\n","        self.protbert = protbert_model\n","        self.scibert = scibert_model\n","        print(\"Freezing BERT parameters...\")\n","        for param in self.protbert.parameters():\n","            param.requires_grad = False\n","        for param in self.scibert.parameters():\n","            param.requires_grad = False\n","        print(\"BERT parameters frozen.\")\n","\n","        self.lstm_hidden_size = hidden_size\n","\n","        self.protbert_lstm = nn.LSTM(input_size=self.protbert.config.hidden_size,\n","                                     hidden_size=self.lstm_hidden_size,\n","                                     batch_first=True, num_layers=num_lstm_layers,\n","                                     dropout=lstm_dropout if num_lstm_layers > 1 else 0,\n","                                     bidirectional=True)\n","        print(f\"ProtBERT BiLSTM: input_size={self.protbert.config.hidden_size}, hidden_size_per_direction={self.lstm_hidden_size}\")\n","\n","        self.scibert_lstm = nn.LSTM(input_size=self.scibert.config.hidden_size,\n","                                    hidden_size=self.lstm_hidden_size,\n","                                    batch_first=True, num_layers=num_lstm_layers,\n","                                    dropout=lstm_dropout if num_lstm_layers > 1 else 0,\n","                                    bidirectional=True)\n","        print(f\"SciBERT BiLSTM: input_size={self.scibert.config.hidden_size}, hidden_size_per_direction={self.lstm_hidden_size}\")\n","\n","\n","        self.numeric_fc_output_dim = self.lstm_hidden_size * 2\n","        self.numeric_fc = nn.Linear(num_numeric_features, self.numeric_fc_output_dim)\n","\n","        combined_input_dim = self.numeric_fc_output_dim * (len_prot_cols + len_sci_cols + 1)\n","        print(combined_input_dim)\n","        self.combined_fc = nn.Sequential(\n","            nn.Linear(combined_input_dim, 128),\n","            nn.ReLU(),\n","            nn.Dropout(final_dropout),\n","            nn.Linear(128, output_size)\n","        )\n","\n","    def forward(self,\n","                input_ids_prot: torch.Tensor, attention_mask_prot: torch.Tensor,\n","                input_ids_sci_list: list[torch.Tensor], attention_mask_sci_list: list[torch.Tensor],\n","                numeric_features: torch.Tensor\n","               ) -> torch.Tensor:\n","\n","        \"\"\"\n","        Performs a forward pass by extracting sequence embeddings from BERT models, processing them with BiLSTMs,\n","        Combines the outputs with numeric features.\n","        Predicts the target through a fully connected regression head.\n","        \"\"\"\n","        try:\n","            prot_outputs = self.protbert(input_ids=input_ids_prot, attention_mask=attention_mask_prot)\n","            prot_embeddings = prot_outputs.last_hidden_state\n","            _ , (prot_hidden_n, _) = self.protbert_lstm(prot_embeddings)\n","            prot_pooled = torch.cat((prot_hidden_n[-2, :, :], prot_hidden_n[-1, :, :]), dim=1)\n","        except Exception as e:\n","            print(f\"!! ERROR during ProtBERT processing: {e}\")\n","            raise e\n","\n","        sci_pooled_list = []\n","        if len(input_ids_sci_list) != len_sci_cols or len(attention_mask_sci_list) != len_sci_cols:\n","            raise ValueError(f\"Expected {len_sci_cols} SciBERT inputs, but got {len(input_ids_sci_list)}\")\n","\n","        for i in range(len_sci_cols):\n","            try:\n","                input_ids_sci = input_ids_sci_list[i]\n","                attention_mask_sci = attention_mask_sci_list[i]\n","                sci_outputs = self.scibert(input_ids=input_ids_sci, attention_mask=attention_mask_sci)\n","                sci_embeddings = sci_outputs.last_hidden_state\n","                _ , (sci_hidden_n, _) = self.scibert_lstm(sci_embeddings)\n","                sci_pooled = torch.cat((sci_hidden_n[-2, :, :], sci_hidden_n[-1, :, :]), dim=1)\n","                sci_pooled_list.append(sci_pooled)\n","            except Exception as e:\n","                print(f\"!! ERROR during SciBERT processing (input {i+1}): {e}\")\n","                print(f\"   Input ID shape: {input_ids_sci.shape}\")\n","                print(f\"   Input Mask shape: {attention_mask_sci.shape}\")\n","                if 'sci_embeddings' in locals(): print(f\"   Embeddings shape: {sci_embeddings.shape}\")\n","                raise e\n","\n","        try:\n","            numeric_out = self.numeric_fc(numeric_features)\n","        except Exception as e:\n","            print(f\"!! ERROR during Numeric FC processing: {e}\")\n","            print(f\"   Input numeric_features shape: {numeric_features.shape}\")\n","            raise e\n","\n","        try:\n","            shapes_ok = prot_pooled.shape[1] == numeric_out.shape[1]\n","            for idx, sp in enumerate(sci_pooled_list):\n","                if sp.shape[1] != prot_pooled.shape[1]:\n","                     shapes_ok = False\n","\n","            if not shapes_ok: print(\"!! WARNING: Hidden dimensions seem inconsistent before concatenation!\")\n","            combined = torch.cat([prot_pooled] + sci_pooled_list + [numeric_out], dim=1)\n","        except Exception as e:\n","            print(f\"!! ERROR during concatenation: {e}\")\n","            print(f\"  Shape prot_pooled: {prot_pooled.shape}\")\n","            for idx, sp in enumerate(sci_pooled_list): print(f\"  Shape sci_pooled_{idx}: {sp.shape}\")\n","            print(f\"  Shape numeric_out: {numeric_out.shape}\")\n","            raise e\n","\n","        try:\n","            x = combined\n","            x = self.combined_fc[0](x)\n","            x = self.combined_fc[1](x)\n","            x = self.combined_fc[2](x)\n","            x = self.combined_fc[3](x)\n","            output = x\n","        except Exception as e:\n","            print(f\"!! ERROR during combined_fc pass: {e}\")\n","            print(f\"   Input shape to combined_fc was: {combined.shape}\")\n","            if 'x' in locals() and x.shape != combined.shape: print(f\"   Shape *after* first linear layer (if reached): {x.shape}\")\n","            raise e\n","\n","        return output"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jYdTQJY6NA6B"},"outputs":[],"source":["class MultiInputDataset(Dataset):\n","    def __init__(self, dataframe,\n","                 prot_token_col: str,\n","                 sci_text_cols: list[str],\n","                 scibert_tokenizer: AutoTokenizer,\n","                 scibert_max_len: int,\n","                 numeric_col_scaled: str,\n","                 target_col_scaled: str,\n","                 target_col_original: str):\n","\n","        self.dataframe = dataframe\n","        self.prot_token_col = prot_token_col\n","\n","        if len(sci_text_cols) != len_sci_cols:\n","            raise ValueError(f\"Expected {len_sci_cols} sci_token_cols, but got {len(sci_text_cols)}\")\n","        self.sci_text_cols = sci_text_cols\n","        self.scibert_tokenizer = scibert_tokenizer\n","        self.scibert_max_len = scibert_max_len\n","\n","        self.numeric_col_scaled = numeric_col_scaled\n","        self.target_col_scaled = target_col_scaled\n","        self.target_col_original = target_col_original\n","\n","        required_cols = [prot_token_col] + sci_text_cols + [numeric_col_scaled, target_col_scaled, target_col_original]\n","        missing_cols = [col for col in required_cols if col not in dataframe.columns]\n","        if missing_cols:\n","            raise ValueError(f\"DataFrame is missing required columns: {missing_cols}\")\n","\n","    def __len__(self):\n","        \"\"\"\n","        Returns the number of samples in the dataset.\n","        \"\"\"\n","        return len(self.dataframe)\n","\n","    def __getitem__(self, idx):\n","        \"\"\"\n","        Retrieves a single sample by index with all correpsonding data.\n","        \"\"\"\n","        if idx >= len(self.dataframe):\n","             raise IndexError(f\"Index {idx} out of bounds for dataframe with length {len(self.dataframe)}\")\n","        try:\n","            item = self.dataframe.iloc[idx]\n","            prot_token_data = item[self.prot_token_col]\n","            if not isinstance(prot_token_data, (dict, BatchEncoding)):\n","                 raise TypeError(f\"Expected a dict in column '{self.prot_token_col}' at index {idx}, but got {type(prot_token_data)}\")\n","            input_ids_prot = torch.tensor(prot_token_data['input_ids'], dtype=torch.long)\n","            attention_mask_prot = torch.tensor(prot_token_data['attention_mask'], dtype=torch.long)\n","\n","            input_ids_sci_list = []\n","            attention_mask_sci_list = []\n","            for i, col_name in enumerate(self.sci_text_cols):\n","                text = item[col_name]\n","                text = str(text) if pd.notna(text) and text is not None else \"\"\n","                sci_token_data = self.scibert_tokenizer(\n","                    text,\n","                    add_special_tokens=True,\n","                    padding='max_length',\n","                    max_length=self.scibert_max_len,\n","                    truncation=True,\n","                    return_attention_mask=True,\n","                    return_tensors='pt')\n","\n","                sci_ids = sci_token_data['input_ids'].squeeze(0)\n","                sci_mask = sci_token_data['attention_mask'].squeeze(0)\n","\n","                input_ids_sci_list.append(sci_ids)\n","                attention_mask_sci_list.append(sci_mask)\n","\n","            numeric_features_data = item[self.numeric_col_scaled]\n","            if isinstance(numeric_features_data, torch.Tensor):\n","                numeric_features = numeric_features_data.float()\n","            else:\n","                numeric_features = torch.tensor(item[self.numeric_col_scaled], dtype=torch.float)\n","\n","            targets_scaled_data = item[self.target_col_scaled]\n","            if isinstance(targets_scaled_data, torch.Tensor):\n","                targets_scaled = targets_scaled_data.float()\n","            else:\n","                targets_scaled = torch.tensor(targets_scaled_data, dtype=torch.float)\n","\n","            targets_original_data = item[self.target_col_original]\n","            if isinstance(targets_original_data, torch.Tensor):\n","                targets_original = targets_original_data.float()\n","            else:\n","                targets_original = torch.tensor(targets_original_data, dtype=torch.float)\n","\n","            return {\n","                'input_ids_prot': input_ids_prot,\n","                'attention_mask_prot': attention_mask_prot,\n","                'input_ids_sci_list': input_ids_sci_list,\n","                'attention_mask_sci_list': attention_mask_sci_list,\n","                'numeric_features': numeric_features,\n","                'targets_scaled': targets_scaled,\n","                'targets_original': targets_original\n","            }\n","        except KeyError as e:\n","            print(f\"Error accessing key in DataFrame or tokenized dict at index {idx}: {e}\")\n","            print(f\"Ensure column '{self.prot_token_col}' and columns in {self.sci_token_cols} exist.\")\n","            print(f\"Also ensure the dictionaries within these columns contain 'input_ids' and 'attention_mask'.\")\n","            raise\n","        except TypeError as e:\n","             print(f\"Type error processing item at index {idx}: {e}\")\n","             raise\n","        except Exception as e:\n","            print(f\"Generic error processing item at index {idx}: {e}\")\n","            raise"]},{"cell_type":"markdown","metadata":{"id":"oMgZgsh-NA6C"},"source":["### Continue with general part"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D4rzWpSmNA6C"},"outputs":[],"source":["# Creating Train and Validation Datasets\n","\n","print(\"Creating Datasets and Dataloaders...\")\n","if model_chosen == \"ESM2\":\n","    train_dataset = ProteinDatasetESM2(train_df, text_cols=dataset_text_columns, tokenizer=tokenizer,\n","                                   num_feat_col=scaled_numeric_col_name,\n","                                   scaled_target_col=scaled_target_col_name,\n","                                   original_target_col=original_target_col_name)\n","\n","    val_dataset = ProteinDatasetESM2(val_df, text_cols=dataset_text_columns, tokenizer=tokenizer,\n","                                 num_feat_col=scaled_numeric_col_name,\n","                                 scaled_target_col=scaled_target_col_name,\n","                                 original_target_col=original_target_col_name)\n","\n","if model_chosen == \"BERT\":\n","    train_dataset = ProteinDatasetBERT(train_df, prot_text_cols=prot_col_list, sci_text_cols=sci_col_list,\n","                                   prot_tokenizer=prot_tokenizer, sci_tokenizer=sci_tokenizer, num_feat_col=scaled_numeric_col_name,\n","                                   scaled_target_col=scaled_target_col_name, original_target_col=original_target_col_name)\n","    val_dataset = ProteinDatasetBERT(val_df, prot_text_cols=prot_col_list, sci_text_cols=sci_col_list,\n","                                 prot_tokenizer=prot_tokenizer, sci_tokenizer=sci_tokenizer, num_feat_col=scaled_numeric_col_name,\n","                                 scaled_target_col=scaled_target_col_name, original_target_col=original_target_col_name)\n","\n","elif model_chosen == \"BiLSTM\":\n","    train_dataset = MultiInputDataset(dataframe=train_df, prot_token_col=prot_col_list[0], sci_text_cols=sci_col_list,\n","                                      scibert_tokenizer=sci_tokenizer, scibert_max_len=512,\n","                                      numeric_col_scaled=scaled_numeric_col_name, target_col_scaled=scaled_target_col_name,\n","                                      target_col_original=original_target_col_name)\n","\n","    val_dataset = MultiInputDataset(dataframe=val_df, prot_token_col=prot_col_list[0],\n","                                    sci_text_cols=sci_col_list, scibert_tokenizer=sci_tokenizer,\n","                                    scibert_max_len=512, numeric_col_scaled=scaled_numeric_col_name,\n","                                    target_col_scaled=scaled_target_col_name, target_col_original=original_target_col_name)\n","\n","if model_chosen == \"BiLSTM\":\n","    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=None)\n","    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=None)\n","else:\n","    train_dataloader = DataLoader(train_dataset, collate_fn=collate_fn, batch_size=batch_size, shuffle=True)\n","    val_dataloader = DataLoader(val_dataset, collate_fn=collate_fn, batch_size=batch_size, shuffle=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J3hBx8tVzvje"},"outputs":[],"source":["num_numeric_features = len_numeric\n","num_tokenized_cols = len(dataset_text_columns)\n","output_size = len_targets\n","print(f\"Numeric features: {num_numeric_features}, Tokenized cols: {num_tokenized_cols}, Output size: {output_size}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K_T1QQGSNA6D"},"outputs":[],"source":["# Defining model wrapper for training.\n","\n","if model_chosen == \"ESM2\":\n","    model_wrapper = AddedSubLayer(pretrained_model=model,\n","                        output_size=output_size,\n","                        num_numeric_features=num_numeric_features,\n","                        num_tokenized_cols=num_tokenized_cols)\n","\n","elif model_chosen == \"BERT\":\n","    model_wrapper = BertModel(prot_model=prot_model, sci_model=sci_model,\n","                              num_numeric_features=num_numeric_features,\n","                              output_size=output_size, num_prot_cols=len(prot_col_list),\n","                              num_sci_cols=len(sci_col_list))\n","\n","elif model_chosen == \"BiLSTM\":\n","    model_wrapper = MultiInputBiLSTMRegressor(protbert_model=prot_model, scibert_model=sci_model,\n","                                              hidden_size=256, num_lstm_layers=2,\n","                                              lstm_dropout=0.2, final_dropout=0.3,\n","                                              num_numeric_features=len_numeric, output_size=output_size)\n","\n","model_wrapper.to(device)\n","print(f\"Model instantiated and moved to {device}.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5Mz6WV9qNA6E"},"outputs":[],"source":["# Initializing the best optimizers for each model.\n","\n","if model_chosen == \"ESM2\":\n","    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model_wrapper.parameters()), lr=learning_rate, weight_decay=1e-4)\n","elif model_chosen == \"BERT\":\n","    optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model_wrapper.parameters()), lr=learning_rate, weight_decay=1e-4)\n","elif model_chosen == \"BiLSTM\":\n","    optimizer = optim.AdamW(model_wrapper.parameters(), lr=learning_rate, weight_decay=0.01)\n","\n","loss_fn = nn.MSELoss()\n","metrics_calculator = MetricsCalculatorUpd(scaler=target_scaler)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wt0zpBkGNA6E"},"outputs":[],"source":["# Setting by Learning Rate Scheduler and evaluation parameters.\n","\n","scheduler = ReduceLROnPlateau(optimizer = optimizer, mode='min', factor=scheduler_factor,\n","                              patience=scheduler_patience_value, min_lr=min_learning_rate)\n","print_prediction_comparison_epoch = True\n","best_val_loss = float('inf')\n","epochs_no_improve = 0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zSDzzDyrNA6F"},"outputs":[],"source":["# Stating correct saving paths for metrics and models parameters.\n","\n","if model_chosen == \"ESM2\":\n","    save_dir = f\"{main_path}Models_Artifacts_{database}/ESM2_{df_state}_text_50_epochs_Mean_Pooling\"\n","\n","    os.makedirs(save_dir, exist_ok=True)\n","    tokenizer_path = os.path.join(save_dir, 'tokenizer') # Tokenizer saves to a directory\n","    metrics_path = os.path.join(save_dir, 'metrics_df_esm')\n","\n","elif model_chosen == \"BERT\":\n","    save_dir = f'{main_path}Models_Artifacts_{database}/BERT_{df_state}_text_50_epochs_Mean_Pooling'\n","    os.makedirs(save_dir, exist_ok=True)\n","    prot_tokenizer_path = os.path.join(save_dir, 'prot_tokenizer')\n","    sci_tokenizer_path = os.path.join(save_dir, 'sci_tokenizer')\n","    metrics_path = os.path.join(save_dir, 'metrics_df_bert')\n","\n","elif model_chosen == \"BiLSTM\":\n","    save_dir = f'{main_path}Models_Artifacts_{database}/MultiInputBiLSTM_{df_state}_text_50_epochs_Mean_Pooling'\n","    os.makedirs(save_dir, exist_ok=True)\n","    prot_tokenizer_path = os.path.join(save_dir, 'prot_tokenizer')\n","    sci_tokenizer_path = os.path.join(save_dir, 'sci_tokenizer')\n","    metrics_path = os.path.join(save_dir, 'metrics_df_lstm')\n","\n","numeric_scaler_path = os.path.join(save_dir, 'numeric_feature_scaler.pkl')\n","target_scaler_path = os.path.join(save_dir, 'target_scaler.pkl')\n","\n","print(f\"Model artifacts will be saved to: {os.path.abspath(save_dir)}\")"]},{"cell_type":"markdown","metadata":{"id":"Uk5ytLkZNA6G"},"source":["### Training and Saving"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"VVRsvBlDzvvY","jupyter":{"outputs_hidden":true}},"outputs":[],"source":["# Training and Evaluation are performed in this chunk.\n","# The best model is saved during the validation if new loss < old loss (for validation).\n","# Gradient clipping is applied along with early to prevent overfitting and exploding gradients.\n","# Metrics are gathered in the end to be saved further on to corresponging save_dir directories.\n","\n","start_time = time.time()\n","print(\"Starting training...\")\n","\n","for epoch in range(num_epochs):\n","    print(f\"\\n--- Epoch {epoch + 1}/{num_epochs} ---\")\n","    current_lr = optimizer.param_groups[0]['lr']\n","    print(f\"Current Learning Rate: {current_lr:.2e}\")\n","\n","    model_wrapper.train()\n","    train_loop = tqdm(train_dataloader, desc=f\"Epoch {epoch+1} Training\", leave=False)\n","    for batch_num, batch in enumerate(train_loop):\n","        if model_chosen == \"BiLSTM\":\n","            input_ids_prot = batch['input_ids_prot'].to(device)\n","            attention_mask_prot = batch['attention_mask_prot'].to(device)\n","            input_ids_sci_list = [tensor.to(device) for tensor in batch['input_ids_sci_list']]\n","            attention_mask_sci_list = [tensor.to(device) for tensor in batch['attention_mask_sci_list']]\n","        else:\n","            input_ids_list = [tensor.to(device) for tensor in batch['input_ids']]\n","            attention_mask_list = [tensor.to(device) for tensor in batch['attention_mask']]\n","\n","        numeric_features = batch['numeric_features'].to(device)\n","        targets_scaled = batch['targets_scaled'].to(device)\n","        targets_original = batch['targets_original'].to(device)\n","\n","        optimizer.zero_grad()\n","        if model_chosen == \"BiLSTM\":\n","            predictions_scaled = model_wrapper(input_ids_prot=input_ids_prot,\n","                                       attention_mask_prot=attention_mask_prot,\n","                                       input_ids_sci_list=input_ids_sci_list,\n","                                       attention_mask_sci_list=attention_mask_sci_list,\n","                                       numeric_features=numeric_features)\n","        else:\n","            predictions_scaled = model_wrapper(input_ids_list, attention_mask_list, numeric_features)\n","\n","        loss = loss_fn(predictions_scaled, targets_scaled)\n","\n","        loss.backward()\n","        if model_chosen == \"ESM2\":\n","            torch.nn.utils.clip_grad_norm_(model_wrapper.parameters(), max_norm=gradient_clip_value)\n","        else:\n","            torch.nn.utils.clip_grad_norm_(filter(lambda p: p.requires_grad, model_wrapper.parameters()), max_norm=gradient_clip_value)\n","\n","        optimizer.step()\n","\n","        metrics_calculator.update_train_batch(loss.item(), numeric_features.size(0))\n","        train_loop.set_postfix(loss=f\"{loss.item():.4f}\")\n","\n","        if batch_num % 100 == 0:\n","             current_avg_loss = metrics_calculator.epoch_train_loss / metrics_calculator.epoch_train_samples if metrics_calculator.epoch_train_samples else 0\n","             print(f\"  Epoch {epoch+1}, Batch {batch_num}/{len(train_dataloader)}, Current Avg Train Loss (scaled): {current_avg_loss:.6f}\")\n","\n","\n","    model_wrapper.eval()\n","    with torch.no_grad():\n","        val_loop = tqdm(val_dataloader, desc=f\"Epoch {epoch+1} Validation\", leave=False, unit=\"batch\")\n","        for batch_num, batch in enumerate(val_loop):\n","            if model_chosen == \"BiLSTM\":\n","                input_ids_prot = batch['input_ids_prot'].to(device)\n","                attention_mask_prot = batch['attention_mask_prot'].to(device)\n","                input_ids_sci_list = [tensor.to(device) for tensor in batch['input_ids_sci_list']]\n","                attention_mask_sci_list = [tensor.to(device) for tensor in batch['attention_mask_sci_list']]\n","            else:\n","                input_ids_list = [tensor.to(device) for tensor in batch['input_ids']]\n","                attention_mask_list = [tensor.to(device) for tensor in batch['attention_mask']]\n","\n","            numeric_features = batch['numeric_features'].to(device)\n","            targets_scaled = batch['targets_scaled'].to(device)\n","            targets_original = batch['targets_original'].to(device)\n","\n","            if model_chosen == \"BiLSTM\":\n","                predictions_scaled = model_wrapper(input_ids_prot=input_ids_prot,\n","                                           attention_mask_prot=attention_mask_prot,\n","                                           input_ids_sci_list=input_ids_sci_list,\n","                                           attention_mask_sci_list=attention_mask_sci_list,\n","                                           numeric_features=numeric_features)\n","            else:\n","                predictions_scaled = model_wrapper(input_ids_list, attention_mask_list, numeric_features)\n","\n","            val_loss_batch = loss_fn(predictions_scaled, targets_scaled)\n","            metrics_calculator.update_val_batch(val_loss_batch.item(),\n","                                                predictions_scaled,\n","                                                targets_scaled,\n","                                                targets_original,\n","                                                numeric_features.size(0))\n","            val_loop.set_postfix(val_loss=f\"{val_loss_batch.item():.4f}\")\n","\n","            if print_prediction_comparison_epoch and batch_num == 0:\n","                predictions_original = target_scaler.inverse_transform(predictions_scaled.cpu().numpy())\n","                targets_original_np = targets_original.cpu().numpy()\n","\n","                print(\"\\n--- Prediction vs. Actual (Original Scale) ---\")\n","                print(\"First few examples from the first validation batch:\")\n","                for i in range(min(5, len(predictions_original))):\n","                    print(f\"Example {i+1}:\")\n","                    print(f\"Prediction: {[f'{p:.2f}' for p in predictions_original[i]]}\")\n","                    print(f\"Actual:     {[f'{t:.2f}' for t in targets_original_np[i]]}\")\n","                    diff = predictions_original[i] - targets_original_np[i]\n","                    print(f\"Difference: {[f'{d:.2f}' for d in diff]}\")\n","                print(\"--------------------------------------------\")\n","\n","    train_loss_epoch, val_loss_epoch, \\\n","    val_mse_avg_epoch, val_rmse_avg_epoch, val_r2_avg_epoch, val_rrmse_avg_epoch, \\\n","    list_val_mse_epoch, list_val_rmse_epoch, list_val_r2_epoch, list_val_rrmse_epoch = \\\n","        metrics_calculator.calculate_epoch_metrics(epoch)\n","\n","    print(f\"\\nEpoch {epoch + 1} Summary:\")\n","    print(f\"Train Loss (scaled): {train_loss_epoch:.6f}\")\n","    val_loss_str = f\"{val_loss_epoch:.6f}\" if not np.isnan(val_loss_epoch) else \"N/A\"\n","    val_mse_avg_str = f\"{val_mse_avg_epoch:.6f}\" if not np.isnan(val_mse_avg_epoch) else \"N/A\"\n","    val_rmse_avg_str = f\"{val_rmse_avg_epoch:.6f}\" if not np.isnan(val_rmse_avg_epoch) else \"N/A\"\n","    val_r2_avg_str = f\"{val_r2_avg_epoch:.6f}\" if not np.isnan(val_r2_avg_epoch) else \"N/A\"\n","    val_rrmse_avg_str = f\"{val_rrmse_avg_epoch:.6f}\" if not np.isnan(val_rrmse_avg_epoch) else \"N/A\"\n","    print(f\"  Val Loss (scaled):   {val_loss_str}\")\n","    print(f\"  Val MSE (avg):       {val_mse_avg_str}\")\n","    print(f\"  Val RMSE (avg):      {val_rmse_avg_str}\")\n","    print(f\"  Val R^2 (avg):       {val_r2_avg_str}\")\n","    print(f\"  Val RRMSE (avg):       {val_rrmse_avg_str}\")\n","\n","    if list_val_mse_epoch:\n","        num_outputs_calc = len(list_val_mse_epoch)\n","        for i in range(num_outputs_calc):\n","              mse_i_str = f\"{list_val_mse_epoch[i]:.6f}\" if not np.isnan(list_val_mse_epoch[i]) else \"N/A\"\n","              rmse_i_str = f\"{list_val_rmse_epoch[i]:.6f}\" if not np.isnan(list_val_rmse_epoch[i]) else \"N/A\"\n","              r2_i_str = f\"{list_val_r2_epoch[i]:.6f}\" if not np.isnan(list_val_r2_epoch[i]) else \"N/A\"\n","              rrmse_i_str = f\"{list_val_rrmse_epoch[i]:.6f}\" if not np.isnan(list_val_rrmse_epoch[i]) else \"N/A\"\n","    else:\n","        print(\"Per-output metrics could not be calculated.\")\n","\n","    scheduler.step(val_loss_epoch)\n","\n","    if val_loss_epoch < best_val_loss:\n","        print(f\"Validation loss improved ({best_val_loss:.6f} --> {val_loss_epoch:.6f}). Saving model...\")\n","        best_val_loss = val_loss_epoch\n","        # epochs_no_improve = 0\n","        # if model_chosen == \"ESM2\":\n","        #     torch.save(model_wrapper.state_dict(), os.path.join(save_dir, \"ESM_best_model.pth\"))\n","        # elif model_chosen == \"BERT\":\n","        #     torch.save(model_wrapper.state_dict(), os.path.join(save_dir, \"BERT_best_model.pth\"))\n","        # elif model_chosen == \"BiLSTM\":\n","        #     torch.save(model_wrapper.state_dict(), os.path.join(save_dir, \"LSTM_best_model.pth\"))\n","\n","    else:\n","        epochs_no_improve += 1\n","        print(f\"Validation loss did not improve for {epochs_no_improve} epoch(s). Best so far: {best_val_loss:.6f}\")\n","\n","    if epochs_no_improve >= early_stopping_patience_value:\n","        print(f\"\\nEarly stopping triggered after {epoch + 1} epochs.\")\n","        break\n","\n","if epoch == num_epochs - 1:\n","     print(\"\\nTraining finished after reaching max epochs.\")\n","print(f\"Best validation loss achieved: {best_val_loss:.6f}\")\n","\n","print(\"\\nTraining finished.\")\n","end_time = time.time()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zDLVHSyWNA6N"},"outputs":[],"source":["# Time Elapsed during training.\n","\n","elapsed_time = end_time - start_time\n","total_epochs_run = epoch + 1\n","print(f\"Training took: {elapsed_time / 3600:.2f} hours ({elapsed_time:.2f} seconds)\")\n","\n","if total_epochs_run > 0:\n","    print(f\"Average time per epoch: {elapsed_time / total_epochs_run:.2f} seconds\")\n","else:\n","    print(\"No epochs were completed.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"-geuy1kz0H0-","jupyter":{"outputs_hidden":true}},"outputs":[],"source":["# Printing final_metrics to see the trend.\n","\n","final_metrics_df = metrics_calculator.get_metrics_df()\n","print(\"\\n--- Final Metrics Summary ---\")\n","print(final_metrics_df.to_string())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PCZxsngUTJXJ"},"outputs":[],"source":["# Saving all collected data to corresponding folders based on model choice.\n","\n","final_metrics_df.to_csv(metrics_path, sep='\\t', index = False)\n","\n","if model_chosen == \"ESM2\":\n","    print(f\"Saving tokenizer to directory: {tokenizer_path}\")\n","    tokenizer.save_pretrained(tokenizer_path)\n","elif model_chosen == \"BERT\":\n","    print(f\"Saving Prot tokenizer to directory: {prot_tokenizer_path}\")\n","    prot_tokenizer.save_pretrained(prot_tokenizer_path)\n","    print(f\"Saving Sci tokenizer to directory: {sci_tokenizer_path}\")\n","    sci_tokenizer.save_pretrained(sci_tokenizer_path)\n","\n","print(f\"Saving numeric feature scaler to: {numeric_scaler_path}\")\n","joblib.dump(numeric_scaler, numeric_scaler_path)\n","\n","print(f\"Saving target value scaler to: {target_scaler_path}\")\n","joblib.dump(target_scaler, target_scaler_path)\n","\n","print(\"\\nInference Artifacts are saved successfully\")"]},{"cell_type":"markdown","metadata":{"id":"RQXdkghXlmEY"},"source":["# Inference (for the BEST model) and visualizations"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fwo3E0Av5ze3"},"outputs":[],"source":["# Specify columns you want (at least, MUST specify 'Sequence' for inference to work!)\n","# Leave \"<UNK>\" for unspecified text columns.\n","# Leave None for unspecified numeric columns (do not specify 'Total Length (AA)' and 'Contour Length [nm]', leave None).\n","\n","text_inputs_dict = {\n","    'Sequence': \"RLDAPSQIEVKDVTDTTALITWFKPLAEIDGIELTYGIKDVPGDRTTIDLTEDENQYSIGNLKPDTEYEVSLISRRGDMSSNPAKETFTT\",\n","    'Name': \"FIBRONECTIN TYPE III DOMAIN FROM TENASCIN\",\n","    'SCOP annotation': \"<UNK>\",\n","    'Experimental Conditions': \"pH = 7\",\n","    'Organism': \"Homo sapiens\",\n","    'Classification': \"CELL ADHESION PROTEIN\",\n","    'Technique': \"<UNK>\",\n","    'Pulling Mode': \"<UNK>\",\n","    'Unfolding Pathway': \"<UNK>\",\n","    'PDB_UniProt': \"1TEN\"\n","}\n","\n","numeric_inputs_dict = {\n","    'Highest unfolding forces/ Clamp forces [pN]': np.nan,\n","    'Standard Deviation of force [pN]':np.nan,\n","    'Total Length (AA)': np.nan,\n","    'Pulling Start': np.nan,\n","    'Pulling End': np.nan,\n","    'Velocity [nm/s]': np.nan,\n","    'Contour Length [nm]': np.nan\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"52l1yv6v8bvF"},"outputs":[],"source":["# Specifying target variables to predict\n","\n","target_columns = [\"ΔG [kBT]\", \"Xu [nm]\", \"Koff [s-¹]\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RC8bRtzqOw7t"},"outputs":[],"source":["# Redefining ESM-2 AddedSublayer (for this section to work independently from Models section).\n","\n","class AddedSubLayer(nn.Module):\n","    def __init__(self, pretrained_model, num_numeric_features, output_size, num_tokenized_cols):\n","        super(AddedSubLayer, self).__init__()\n","        self.esm2 = pretrained_model\n","        self.esm2_hidden_size = pretrained_model.config.hidden_size\n","        self.num_tokenized_cols = num_tokenized_cols\n","        self.fc_input_size = self.esm2_hidden_size * self.num_tokenized_cols + num_numeric_features\n","\n","        self.fc = nn.Linear(self.fc_input_size, output_size)\n","        self.dropout = nn.Dropout(p=0.1)\n","\n","    def _mean_pooling(self, model_output, attention_mask):\n","        token_embeddings = model_output.last_hidden_state\n","        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n","        sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n","        sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n","        return sum_embeddings / sum_mask\n","\n","    def forward(self, input_ids_list, attention_mask_list, numeric_features):\n","        pooled_outputs = []\n","        assert len(input_ids_list) == self.num_tokenized_cols, f\"Expected {self.num_tokenized_cols} input sequences, got {len(input_ids_list)}\"\n","        assert len(attention_mask_list) == self.num_tokenized_cols, f\"Expected {self.num_tokenized_cols} attention masks, got {len(attention_mask_list)}\"\n","\n","        for i in range(self.num_tokenized_cols):\n","            outputs = self.esm2(input_ids=input_ids_list[i], attention_mask=attention_mask_list[i])\n","            pooled_output = self._mean_pooling(outputs, attention_mask_list[i])\n","            pooled_outputs.append(pooled_output)\n","        concatenated_pooled_output = torch.cat(pooled_outputs, dim=1)\n","\n","        combined_features = torch.cat((concatenated_pooled_output, numeric_features), dim=1)\n","        combined_features = self.dropout(combined_features)\n","        logits = self.fc(combined_features)\n","        predictions_scaled = torch.sigmoid(logits)\n","        return predictions_scaled"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h5GZ5Ote1GIf"},"outputs":[],"source":["# Inference part, defined only for MechanoProDB due to incomplete received dataset of ProThermDB and only for ESM-2.\n","\n","database = 'mechano'\n","artifacts_path = f\"{main_path}Models_Artifacts_{database}/ESM2_all_text_50_epochs_Mean_Pooling/\"\n","target_scaler_path = f\"{artifacts_path}target_scaler.pkl\"\n","numeric_scaler_path = f\"{artifacts_path}numeric_feature_scaler.pkl\"\n","model_state_path = f\"{artifacts_path}ESM_best_model.pth\"\n","tokenizer_path = f\"{artifacts_path}tokenizer\"\n","\n","loaded_target_scaler = joblib.load(target_scaler_path)\n","output_size = loaded_target_scaler.n_features_in_\n","print(f\"Scaler expects {output_size} target variables.\")\n","loaded_numeric_scaler = joblib.load(numeric_scaler_path)\n","total_num_features = loaded_numeric_scaler.n_features_in_\n","print(f\"Scaler expects {total_num_features} numeric features.\")\n","loaded_tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n","\n","max_token_length = 1024\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","base_model_name = \"facebook/esm2_t6_8M_UR50D\"\n","num_expected_text_cols = 10\n","\n","try:\n","    print(f\"Loading base model: {base_model_name}...\")\n","    base_esm_model = AutoModel.from_pretrained(base_model_name).to(device)\n","    print(\"Base model loaded.\")\n","\n","    model_inference = AddedSubLayer(\n","        pretrained_model=base_esm_model,\n","        num_numeric_features=total_num_features,\n","        output_size=output_size,\n","        num_tokenized_cols=num_expected_text_cols\n","    ).to(device)\n","\n","    model_inference.load_state_dict(torch.load(model_state_path, map_location=device))\n","    model_inference.eval()\n","    print(\"Wrapper model instantiated and state loaded successfully.\")\n","except Exception as e:\n","    print(f\"Error loading model state from {model_state_path}: {e}\")\n","    print(\"Check base_model_name, AddedSubLayer definition, and saved state compatibility.\")\n","    exit()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c0SrYU2Q2qsz"},"outputs":[],"source":["def predict_single_instance(text_inputs, numeric_inputs, model, tokenizer, numeric_scaler, target_scaler, max_len, device, num_text_cols):\n","    \"\"\"\n","    Makes a prediction for a single instance of data.\n","    \"\"\"\n","    if len(text_inputs) != num_text_cols:\n","        raise ValueError(f\"Expected {num_text_cols} text inputs, but got {len(text_inputs)}\")\n","    if len(numeric_inputs) != numeric_scaler.n_features_in_:\n","         raise ValueError(f\"Expected {numeric_scaler.n_features_in_} numeric inputs, but got {len(numeric_inputs)}\")\n","\n","    model.eval()\n","\n","    input_ids_list = []\n","    attention_mask_list = []\n","    for text in text_inputs:\n","        encoding = tokenizer.encode_plus(\n","            text,\n","            add_special_tokens=True,\n","            max_length=max_len,\n","            padding='max_length',\n","            truncation=True,\n","            return_attention_mask=True,\n","            return_tensors='pt',\n","        )\n","        input_ids_list.append(encoding['input_ids'].to(device))\n","        attention_mask_list.append(encoding['attention_mask'].to(device))\n","\n","    numeric_inputs_np = np.array(numeric_inputs).reshape(1, -1)\n","    scaled_numeric_features = numeric_scaler.transform(numeric_inputs_np)\n","    numeric_features_tensor = torch.tensor(scaled_numeric_features, dtype=torch.float32).to(device)\n","\n","    with torch.no_grad():\n","        scaled_predictions = model(\n","            input_ids_list=input_ids_list,\n","            attention_mask_list=attention_mask_list,\n","            numeric_features=numeric_features_tensor\n","        )\n","\n","    scaled_predictions_np = scaled_predictions.cpu().numpy()\n","    if scaled_predictions_np.ndim == 1:\n","        scaled_predictions_np = scaled_predictions_np.reshape(1, -1)\n","\n","    final_predictions = target_scaler.inverse_transform(scaled_predictions_np)\n","\n","    return final_predictions.flatten()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2s_U2y1D3Ncu"},"outputs":[],"source":["# Filling non-specified numeric values by means of corresponding columns from MechanoProDB.\n","# Automatic filling of 'Total Length' and 'Contour Length' from 'Sequence'\n","\n","numeric_inputs_dict['Total Length (AA)'] = len(text_inputs_dict['Sequence'])\n","numeric_inputs_dict['Contour Length [nm]'] = 0.35 * numeric_inputs_dict['Total Length (AA)']\n","numeric_inputs_dict['Pulling Start'] = 1 if np.isnan(numeric_inputs_dict['Pulling Start']) else numeric_inputs_dict['Pulling Start']\n","numeric_inputs_dict['Pulling End'] = numeric_inputs_dict['Total Length (AA)'] if np.isnan(numeric_inputs_dict['Pulling End']) \\\n","                                     else numeric_inputs_dict['Pulling End']\n","numeric_inputs_dict['Highest unfolding forces/ Clamp forces [pN]'] = tokenized_df['Highest unfolding forces/ Clamp forces [pN]'].mean() \\\n","                                                                     if np.isnan(numeric_inputs_dict['Highest unfolding forces/ Clamp forces [pN]']) \\\n","                                                                     else numeric_inputs_dict['Highest unfolding forces/ Clamp forces [pN]']\n","numeric_inputs_dict['Standard Deviation of force [pN]'] = tokenized_df['Standard Deviation of force [pN]'].mean() \\\n","                                                          if np.isnan(numeric_inputs_dict['Standard Deviation of force [pN]']) \\\n","                                                          else numeric_inputs_dict['Standard Deviation of force [pN]']\n","numeric_inputs_dict['Velocity [nm/s]'] = tokenized_df['Velocity [nm/s]'].mean() if np.isnan(numeric_inputs_dict['Velocity [nm/s]']) \\\n","                                         else numeric_inputs_dict['Velocity [nm/s]']\n","\n","text_inputs_dict_values = list(text_inputs_dict.values())\n","temp_numeric_inputs_dict_values = list(numeric_inputs_dict.values())\n","numeric_inputs_dict_values = [round(float(val), 2) for val in temp_numeric_inputs_dict_values]\n","\n","assert len(text_inputs_dict_values) == num_expected_text_cols, \"Incorrect number of text inputs\"\n","assert len(numeric_inputs_dict_values) == total_num_features, \"Incorrect number of numeric inputs\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HynXeLKv3Ni4"},"outputs":[],"source":["# Instantiating the predictions and saving predicted values to corerpsonding variables for future visualizations.\n","\n","try:\n","    predictions = predict_single_instance(text_inputs=text_inputs_dict_values, numeric_inputs=numeric_inputs_dict_values,\n","                                          model=model_inference, tokenizer=loaded_tokenizer, numeric_scaler=loaded_numeric_scaler,\n","                                          target_scaler=loaded_target_scaler, max_len=max_token_length,\n","                                          device=device, num_text_cols=num_expected_text_cols)\n","\n","    print(\"\\n - CHECK YOUR INPUTS -\\n\")\n","    print(\"\\n Text Inputs:\")\n","    for key, value in text_inputs_dict.items():\n","      print(f\"{key}: {value[:50]}...\" if value != \"<UNK>\" else f\"{key}: {value}\")\n","    print()\n","    print(\"\\n Numeric Inputs:\")\n","    for key, val in zip(numeric_inputs_dict.keys(), numeric_inputs_dict_values):\n","      print(f\"{key}: {val}\")\n","\n","\n","    print(\"\\n - PREDICTIONS - \\n\")\n","    print(f\"{target_columns[0]}: {predictions[0]}\")\n","    print(f\"{target_columns[1]}: {predictions[1]}\")\n","    print(f\"{target_columns[2]}: {predictions[2]}\")\n","\n","    delta_G = predictions[0]\n","    Xu = predictions[1]\n","    Koff = predictions[2]\n","    contour_length = numeric_inputs_dict['Contour Length [nm]']\n","    kBT = 4.1\n","\n","except ValueError as ve:\n","    print(f\"Input data validation error: {ve}\")\n","except Exception as e:\n","    print(f\"An error occurred during prediction: {e}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8aWPHNjsloXt"},"outputs":[],"source":["def calculate_G0(x, delta_G, Xu, mu=2/3, theta=0):\n","    \"\"\"\n","    Calculates the free energy G0(x) based on the provided formula\n","    with constants mu = 2/3 and theta = 0.\n","    These parameters are used as a convention.\n","    \"\"\"\n","    if Xu <= 0:\n","        raise ValueError(\"Xu (distance x‡) must be positive.\")\n","\n","    term1 = (delta_G * x) / (mu * Xu)\n","    term2 = (2 * x**2) / Xu\n","    term3 = np.abs(x) + x * np.sin(theta)\n","    term4 = mu / (1 - mu)\n","    term5 = (term2 / term3)**term4\n","    term6 = 1 - (1 - mu) * term5\n","\n","    g0 = term1 * term6\n","\n","    return g0\n","\n","def plot_energy_landscape(delta_G=delta_G, Xu=Xu):\n","    \"\"\"\n","    Visualizes the one-dimensional free energy profile along a reaction coordinate based on given parameters.\n","    Highlights the minimum (Gn) and maximum (Gt) points on the plot.\n","    \"\"\"\n","    x_vals = np.linspace(-2, 2, 1000)\n","    g_vals = calculate_G0(x_vals, delta_G=delta_G, Xu=Xu)\n","\n","    min_idx = argrelextrema(g_vals, np.less)[0]\n","    max_idx = argrelextrema(g_vals, np.greater)[0]\n","\n","    plt.plot(x_vals, g_vals, color='black')\n","    plt.xlabel(\"Reaction Coordinate (nm)\", fontsize=14)\n","    plt.ylabel(\"G₀(x)\", fontsize=14)\n","    plt.title(\"Free Energy Profile\", fontsize=14)\n","    plt.grid(False)\n","    plt.ylim(-1.5 * delta_G, 1.5 * delta_G)\n","    plt.xlim(-1.3 * Xu, 1.3 * Xu)\n","    plt.xticks(fontsize=12)\n","    plt.yticks(fontsize=12)\n","\n","    if len(min_idx) > 0:\n","        x_min = x_vals[min_idx[0]]\n","        y_min = g_vals[min_idx[0]]\n","        plt.plot(x_min, y_min, 'o', color='blue', alpha = 0.5)\n","        plt.text(x_min, y_min - 2, \"$G_n$\", ha='center', va='top', fontsize=14)\n","\n","    if len(max_idx) > 0:\n","        x_max = x_vals[max_idx[0]]\n","        y_max = g_vals[max_idx[0]]\n","        plt.plot(x_max, y_max, 'o', color='red', alpha=0.5)\n","        plt.text(x_max, y_max + 2, \"$G_t$\", ha='center', va='bottom', fontsize=14)\n","\n","    plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aDpSr86vlocK"},"outputs":[],"source":["plot_energy_landscape()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kGrcEB4RHOoO"},"outputs":[],"source":["def heaviside(x):\n","    if x > 0:\n","      return 1\n","    else:\n","      return 0\n","\n","def compute_F_cFirst(delta_G, Xu, Koff, gamma=0.5772, kBT=kBT):\n","    \"\"\"\n","    Computes Ḟ_cFirst using the Kramers’ approximation.\n","    γ ≈ 0.5772 is the Euler–Mascheroni constant from paper.\n","    \"\"\"\n","    numerator = (Koff * kBT * np.exp((delta_G / kBT) + gamma)) / Xu\n","    exponent = -delta_G / kBT\n","    return numerator * (1 - np.exp(exponent))\n","\n","def find_diffusion_coeff(delta_G, Xu, Koff, kBT=kBT):\n","    \"\"\"\n","    Zeta is the diffusion coefficient (ζ)\n","    Computes zeta from known K0 = Koff.\n","    \"\"\"\n","    numerator = 3 * delta_G\n","    denominator = np.pi * Koff * Xu**2\n","    exp_term = np.exp(-delta_G / kBT)\n","    zeta = (numerator / denominator) * exp_term\n","    return zeta\n","\n","def F_first(delta_G, xu, Koff, nu=2/3, kBT=kBT, F_dot=100000):\n","    \"\"\"\n","    Calculates F_first from the paper, needed for the next calculations.\n","    F_dot is the loading rate, can be changed by user if needed (by default is set to 10^5 pN/s).\n","    \"\"\"\n","    F_cFirst = compute_F_cFirst(delta_G, xu, Koff)\n","    zeta = find_diffusion_coeff(delta_G, Xu, Koff)\n","\n","    term_1_prefactor = delta_G / (nu * Xu)\n","    e1_arg = (Koff * kBT) / (Xu * F_dot)\n","    E1 = exp1(e1_arg)\n","    partial_term = (kBT / delta_G) * np.exp(e1_arg)\n","\n","    inner_term_1 = 1 - (np.abs(1 - (partial_term * E1))**nu)\n","    term_1 = term_1_prefactor * (inner_term_1) * heaviside(F_cFirst - F_dot)\n","\n","    sqrt_2F_first = np.sqrt(2 * F_cFirst * Xu * zeta)\n","    sqrt_2F_dot = np.sqrt(2 * F_dot * Xu * zeta)\n","    inner_term_2 = (delta_G / (nu * xu)) - sqrt_2F_first + sqrt_2F_dot\n","    term_2 = inner_term_2 * heaviside(F_dot - F_cFirst)\n","\n","    final_result = term_1 + term_2\n","\n","    return final_result\n","\n","def x_of_f(f, L):\n","    \"\"\"\n","    Calculates the extension of a polymer under a given force, using a semi-empirical force-extension relation, scaled by the contour length.\n","    \"\"\"\n","    if f <= 0:\n","        return np.nan\n","    try:\n","      term1 = 4 / 3\n","      term2 = 4 / (3 * np.sqrt(f + 1))\n","      exp_term = np.exp(np.power(900 / f, 0.25))\n","      term3 = (10 * exp_term) / (np.sqrt(f) * (exp_term - 1)**2)\n","      term4 = f**1.62 / (3.55 + 3.8 * f**2.2)\n","      final_value = (term1 - term2 - term3 + term4) * L\n","      return final_value\n","    except Exception:\n","        return np.nan\n","\n","def find_force_extension_parameters(delta_G=delta_G, Xu=Xu, Koff=Koff, L=contour_length, kBT=kBT, P=0.4):\n","    \"\"\"\n","    Computes the expected extension of a polymer based on molecular energy parameters and worm-like chain (WLC) model approximations.\n","    P is the persistence length in nanometers, conventionally is 0.4\n","    Contour length is taken from the input data.\n","    \"\"\"\n","    F_final_value = F_first(delta_G, Xu, Koff)\n","    f_target = (F_final_value * P) / kBT\n","\n","    x_f = x_of_f(f_target, L)\n","    return x_f\n","\n","def inverse_force_extension_formula(x_target, delta_G=delta_G, Xu=Xu, Koff=Koff, L=contour_length, kBT=kBT, P=0.4):\n","    \"\"\"\n","    Finds the force that corresponds to a given extension by numerically solving the inverse of the force-extension relationship.\n","    \"\"\"\n","    F_final_value = F_first(delta_G, Xu, Koff)\n","    f_target = (F_final_value * P) / kBT\n","\n","    def equation(f):\n","        return x_of_f(f, L) - x_target\n","\n","    f_guess = 0.1\n","    try:\n","        f_solution = fsolve(equation, f_guess, xtol=1e-6)[0]\n","        return f_solution\n","    except Exception:\n","        return np.nan\n","\n","def plot_force_extension_curve():\n","    \"\"\"\n","    Plots the theoretical force-extension curve of a polymer.\n","    \"\"\"\n","    x_val = find_force_extension_parameters()\n","    x_vals = np.linspace(0, x_val, 200)\n","    f_vals = [inverse_force_extension_formula(x) for x in x_vals]\n","\n","    noise_strength = 0.2\n","    f_vals_noisy = f_vals + np.random.normal(0, noise_strength, size=len(f_vals))\n","    plt.plot(x_vals, f_vals_noisy, label='f(x)', color='black', linewidth=1.4)\n","    x_last = x_vals[-1]\n","    y_last = f_vals_noisy[-1]\n","    plt.plot([x_last, x_last], [y_last, 0], color='black', linewidth=1.4)\n","    tail_x = np.linspace(x_last, x_last + 5, 50)\n","    tail_noise = np.random.normal(0, noise_strength, size=tail_x.shape)\n","    plt.plot(tail_x, tail_noise, color='black', linewidth=1.4)\n","\n","    plt.axhline(y=0, color='red', linestyle='--')\n","    plt.title('Force-Extension Curve', fontsize=14)\n","    plt.xlabel('Extension (nm)', fontsize=14)\n","    plt.ylabel('Force (pN)', fontsize=14)\n","    # plt.legend()\n","    plt.grid(False)\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Sl1Uqv6akgqr"},"outputs":[],"source":["plot_force_extension_curve()"]},{"cell_type":"markdown","metadata":{"id":"SjC634fIJI8i"},"source":["# Comparison of results (3 Neural Network Models)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pheY4c2YJRk7"},"outputs":[],"source":["# Loading metrics for MechanoProDB and ProThermDB from their corresponding folders.\n","\n","artifacts_path = f\"{main_path}Models_Artifacts_{database}/\"\n","main_dir = f\"_{df_state}_text_50_epochs_Mean_Pooling/metrics_df_\"\n","\n","metrics_esm = pd.read_csv(artifacts_path + \"ESM2\" + main_dir + \"esm.tsv\", sep='\\t')\n","if mechano:\n","  metrics_bert = pd.read_csv(artifacts_path + \"BERT\" + main_dir + \"bert.tsv\", sep='\\t')\n","  metrics_lstm = pd.read_csv(artifacts_path + \"MultiInputBiLSTM\" + main_dir + \"lstm.tsv\", sep='\\t')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oLivU7q9w3Ed"},"outputs":[],"source":["# Due to non-changing nature of BiLSTM model atfer first epochs, to save the time, last 15 epochs vere average by the last run epoch parameters.\n","\n","if mechano:\n","  last_row = metrics_lstm.iloc[[-1]]\n","  last_15_rows = pd.concat([last_row] * (50 - len(metrics_lstm)) , ignore_index=True)\n","  last_15_rows['epoch'] = range(len(metrics_lstm), 50)\n","  metrics_lstm_extended = pd.concat([metrics_lstm, last_15_rows], ignore_index=True)\n","\n","  epochs = metrics_lstm_extended.index if 'epoch' not in metrics_lstm_extended.columns else metrics_lstm_extended['epoch']\n","else:\n","  epochs = metrics_esm.index if 'epoch' not in metrics_esm.columns else metrics_esm['epoch']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fWb1oseRJRvz"},"outputs":[],"source":["# Plotting Scores for Evaluation Metrics of each model for target variables (both for MechanoProDB and ProThermDB).\n","\n","if mechano:\n","  target_names = [\"ΔG\", \"Xu\", \"Koff\"]\n","else:\n","  target_names = [\"Tm_(C)\"]\n","\n","metrics_names = ['MSE','RMSE','R^2', 'Loss', 'RRMSE']\n","metrics_names_small = ['mse', 'rmse', 'r2', 'loss', 'rrmse']\n","\n","for i in range(len(target_names)):\n","  for j in range(5):\n","    if j != 3:\n","      print(f'{i} --> {j}')\n","      print(metrics_names_small[j])\n","      plt.figure(figsize=(9, 5))\n","      print(f'val_{metrics_names_small[j]}_{i}')\n","      plt.plot(epochs, metrics_esm[f'val_{metrics_names_small[j]}_{i}'], label=f'ESM2', color='red')\n","      if mechano:\n","        plt.plot(epochs, metrics_bert[f'val_{metrics_names_small[j]}_{i}'], label=f'BERT', color='blue')\n","        plt.plot(epochs, metrics_lstm_extended[f'val_{metrics_names_small[j]}_{i}'], label=f'BiLSTM', color='green')\n","      plt.title(f\"Validation Scores for {target_names[i]} \")\n","      plt.xlabel(\"Epoch\")\n","      plt.ylabel(metrics_names[j])\n","      plt.legend()\n","      plt.grid(True, alpha=0.5)\n","      plt.tight_layout()\n","      plt.show()\n","    else:\n","      continue\n","\n","for i in range(5):\n","  if i!= 5:\n","    part = \"_avg\"\n","    if i == 3:\n","      part = \"\"\n","    plt.figure(figsize=(9, 5))\n","    plt.plot(epochs, metrics_esm[f'val_{metrics_names_small[i]}{part}'], label=f'ESM2', color='red')\n","    if mechano:\n","      plt.plot(epochs, metrics_bert[f'val_{metrics_names_small[i]}{part}'], label=f'BERT', color='blue')\n","      plt.plot(epochs, metrics_lstm_extended[f'val_{metrics_names_small[i]}{part}'], label=f'BiLSTM', color='green')\n","    plt.title(f\"Average Validation Scores\")\n","    plt.xlabel(\"Epoch\")\n","    plt.ylabel(metrics_names[i])\n","    plt.legend()\n","    plt.grid(True, alpha=0.5)\n","    plt.tight_layout()\n","    plt.show()"]},{"cell_type":"markdown","metadata":{"id":"UaujpD9jJR0w"},"source":["***"]},{"cell_type":"markdown","metadata":{"id":"NzFwgcL1NA6Z"},"source":["# End of Notebook"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["Bd43FY7g4mKv","VzGzYNTedhn7","RQXdkghXlmEY","SjC634fIJI8i"],"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"}},"nbformat":4,"nbformat_minor":0}